{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeufDKPr9sS_"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DalasNoin/arena/blob/main/w1/attention.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9iXrqhs9wDd"
      },
      "source": [
        "### Create a Transfomer and test it on a toy example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mlWXh8id9sTB"
      },
      "outputs": [],
      "source": [
        "# in colab \n",
        "# ! wget https://raw.githubusercontent.com/DalasNoin/arena/main/w1/shakespeare.py\n",
        "# ! pip install transformers\n",
        "# ! pip install wandb\n",
        "# ! wget https://www.gutenberg.org/files/100/100-0.txt\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"arena week 1\"\n",
        "wandb_key = \"\"\n",
        "keyfile = \"keystore.yaml\"\n",
        "if not wandb_key and os.path.exists(keyfile):\n",
        "    import yaml\n",
        "    keys = yaml.safe_load(open(keyfile,\"r\"))\n",
        "    wandb_key = keys[\"wandb\"]\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "not wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlW4UMtp9sTC",
        "outputId": "66729385-b1e3-4fd1-ad23-4173b398aabe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 64])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q = torch.ones((2,100,64))\n",
        "K = torch.ones((2,90,64))\n",
        "V = torch.ones((2,90,64))\n",
        "\n",
        "\n",
        "def attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
        "    '''\n",
        "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
        "\n",
        "    With this function, you can ignore masking.\n",
        "\n",
        "    Q: shape (batch, target sequence length, embedding dim)\n",
        "    K: shape (batch, source sequence length, embedding dim)\n",
        "    V: shape (batch, source sequence length, embedding dim)\n",
        "    softmax(Q KT/sqrt(d_k))V\n",
        "\n",
        "    Return: shape (same as Q if embedding dim same. batch, target sequence length, output embedding dim)\n",
        "    '''\n",
        "    sqrt_d_k = torch.sqrt(torch.tensor(K.shape[-1]))\n",
        "    query_key = torch.bmm(Q,torch.transpose(K,1,2))\n",
        "    # print(f\"{query_key.shape=} {sqrt_d_k=}\")\n",
        "    result =torch.bmm(softmax(query_key/sqrt_d_k,dim=2), V)\n",
        "    return result\n",
        "\n",
        "attention(Q, K, V).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6GJAW-I9sTD",
        "outputId": "2e98bfac-90f1-42f9-9bcb-b695c7d3d1eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = torch.ones((2,20,64))\n",
        "K = torch.ones((2,10,64))\n",
        "V = torch.ones((2,10,64))\n",
        "\n",
        "def masked_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
        "    '''\n",
        "    Should return the results of self-attention.\n",
        "\n",
        "    You should implement masking for this function. See \"The Decoder Side\" for an explanation of masking.\n",
        "\n",
        "    Q: shape (batch, target sequence length, embedding dim)\n",
        "    K: shape (batch, source sequence length, embedding dim)\n",
        "    V: shape (batch, source sequence length, embedding dim)\n",
        "    I = Q K.T\n",
        "    I.shape = target_len x source_len\n",
        "    softmax((I+mask)/sqrt(d_k))V\n",
        "\n",
        "    Return: shape (same as Q if embedding dim same. batch, target sequence length, output embedding dim)\n",
        "    '''\n",
        "    sqrt_d_k = torch.sqrt(torch.tensor(K.shape[-1]))\n",
        "    target_seq_len = torch.tensor(Q.shape[1])\n",
        "    source_seq_len = torch.tensor(K.shape[1])\n",
        "    triangular = torch.triu(torch.ones((target_seq_len, source_seq_len), dtype=torch.bool), diagonal=1)\n",
        "    # print(triangular)\n",
        "\n",
        "    query_key = torch.bmm(Q, torch.transpose(K,1,2))\n",
        "    masked_query_key = torch.where(triangular, -torch.inf, query_key)\n",
        "    # print(masked_query_key.shape, query_key.shape, triangular.shape)\n",
        "    result =torch.bmm(softmax((masked_query_key)/sqrt_d_k,dim=2), V)\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "result = masked_attention(Q, K, V)\n",
        "print(result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KABaU-mM9sTD"
      },
      "outputs": [],
      "source": [
        "# from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQVh_Q7k9sTE",
        "outputId": "12f81628-b437-4ea8-f2f2-85f2d59afc64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 20, 256])\n"
          ]
        }
      ],
      "source": [
        "Q = torch.ones((2,20,4*64))\n",
        "K = torch.ones((2,10,4*64))\n",
        "V = torch.ones((2,10,4*64))\n",
        "num_heads = 4\n",
        "\n",
        "def multihead_masked_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, num_heads: int, device:str=\"cpu\"):\n",
        "    '''\n",
        "    Implements multihead masked attention on the matrices Q, K and V.\n",
        "\n",
        "    Q: shape (batch, seq, nheads*headsize)\n",
        "    K: shape (batch, seq, nheads*headsize)\n",
        "    V: shape (batch, seq, nheads*headsize)\n",
        "    '''\n",
        "    # do the reshape\n",
        "    \n",
        "    batch, target_seq_len = Q.shape[0:2]\n",
        "    source_seq_len = K.shape[1] \n",
        "    head_size = int(Q.shape[-1]/num_heads)\n",
        "    sqrt_d_k = torch.sqrt(torch.tensor(head_size))\n",
        "    # new_shape = (batch, target_seq_len, num_heads, head_size)\n",
        "    Q = torch.reshape(Q, (batch, target_seq_len, num_heads, head_size))\n",
        "    K = torch.reshape(K, (batch, source_seq_len, num_heads, head_size))\n",
        "    V = torch.reshape(V, (batch, source_seq_len, num_heads, head_size))\n",
        "    # generate mask\n",
        "    triangular = torch.triu(torch.ones((target_seq_len, source_seq_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    \n",
        "    query_key = torch.einsum(\"abcd,aecd->acbe\", Q, K)\n",
        "    masked_query_key = torch.where(triangular, -torch.inf, query_key)\n",
        "    masked_query_key = softmax((masked_query_key)/sqrt_d_k,dim=3)\n",
        "    result = torch.einsum(\"abcd, adbe-> acbe\", masked_query_key, V)\n",
        "    result = torch.reshape(result, (batch, target_seq_len, num_heads * head_size))\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "result = multihead_masked_attention(Q, K, V, num_heads=num_heads)\n",
        "print(result.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uJSNJynz9sTE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000,  0.8000,  1.6000,  2.4000],\n",
              "         [ 3.2000,  4.0000,  4.8000,  5.6000],\n",
              "         [ 6.4000,  7.2000,  8.0000,  8.8000],\n",
              "         [ 9.6000, 10.4000, 11.2000, 12.0000],\n",
              "         [12.8000, 13.6000, 14.4000, 15.2000],\n",
              "         [16.0000, 16.8000, 17.6000, 18.4000],\n",
              "         [19.2000, 20.0000, 20.8000, 21.6000]],\n",
              "\n",
              "        [[22.4000, 23.2000, 24.0000, 24.8000],\n",
              "         [25.6000, 26.4000, 27.2000, 28.0000],\n",
              "         [28.8000, 29.6000, 30.4000, 31.2000],\n",
              "         [32.0000, 32.8000, 33.6000, 34.4000],\n",
              "         [35.2000, 36.0000, 36.8000, 37.6000],\n",
              "         [38.4000, 39.2000, 40.0000, 40.8000],\n",
              "         [41.6000, 42.4000, 43.2000, 44.0000]]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q = torch.arange(2 * 7 * 4).reshape(2, 7, 4).type(torch.float32)\n",
        "K = Q * 0.5\n",
        "V = Q * 0.8\n",
        "num_heads=2\n",
        "multihead_masked_attention(Q, K, V, num_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2TyEhq8w9sTF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 64])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class MultiheadMaskedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    head_size is not in this config, because in our implementation we're assuming num_heads * head_size = hidden_size.\n",
        "hidden_size is also referred to as embedding_dim, or d_\\text{model}d \n",
        "model\n",
        "​\n",
        "  in some material you might have read.\n",
        "    \"\"\"\n",
        "    W_QKV: nn.Linear\n",
        "    W_O: nn.Linear\n",
        "\n",
        "\n",
        "    def __init__(self, hidden_size: int, num_heads: int, device:str=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        self.W_QKV = nn.Linear(hidden_size*3, num_heads*hidden_size*3)\n",
        "        self.W_O = nn.Linear(num_heads*hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        '''\n",
        "        x: shape (batch, seq, hidden_size)\n",
        "\n",
        "        Return: shape (batch, seq, hidden_size)\n",
        "        '''\n",
        "        x = x.repeat((1,1,3)) # repeat trice along dim 2\n",
        "        x = self.W_QKV(x)\n",
        "        #print(f\"{x.shape=} {num_heads=} {self.hidden_size=}\")\n",
        "        Q, K, V = torch.split(x, num_heads*self.hidden_size, 2)\n",
        "        #print(f\"{Q.shape=} {K.shape=} {V.shape=}\")\n",
        "        \n",
        "        Z = multihead_masked_attention(Q, K, V, num_heads=self.num_heads, device=self.device)\n",
        "        #print(f\"{Z.shape=}\")\n",
        "        Z = self.W_O(Z)\n",
        "        return Z\n",
        "\n",
        "num_heads=4\n",
        "hidden_size=64\n",
        "x = torch.ones((2,10,hidden_size))       \n",
        "mma = MultiheadMaskedAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
        "mma(x).shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7agfiLCs9sTF"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TransformerConfig:\n",
        "    '''Constants used throughout your decoder-only transformer model.'''\n",
        "\n",
        "    num_layers: int\n",
        "    # head_size is not in this config, because in our implementation we're assuming num_heads * head_size = hidden_size\n",
        "    num_heads: int\n",
        "    vocab_size: int\n",
        "    # hidden_size is also referred to as embedding_dim, or d_\\text{model}d model in some material you might have read.\n",
        "    hidden_size: int\n",
        "    # max_seq_len is used just to determine the size of the positional encoding matrix.\n",
        "    max_seq_len: int \n",
        "    dropout: float = 0.1\n",
        "    layer_norm_epsilon: float = 1e-05\n",
        "    device: str = \"cpu\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CqiplUqF9sTF"
      },
      "outputs": [],
      "source": [
        "from torch.nn import GELU\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_size: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = dropout\n",
        "        self.mlp_block = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size, 4*self.hidden_size),\n",
        "            GELU(),\n",
        "            nn.Linear(4*self.hidden_size, self.hidden_size),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.mlp_block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wBPTdYRG9sTG"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "        self.layernorm1 = nn.LayerNorm(normalized_shape=self.config.hidden_size,eps=self.config.layer_norm_epsilon)\n",
        "        self.mma = MultiheadMaskedAttention(hidden_size=self.config.hidden_size,\n",
        "                                            num_heads=self.config.num_heads,\n",
        "                                            device=self.device)\n",
        "        self.layernorm2 = nn.LayerNorm(normalized_shape=self.config.hidden_size,eps=self.config.layer_norm_epsilon)\n",
        "        self.mlp = MLP(hidden_size=self.config.hidden_size,\n",
        "                        dropout=self.config.dropout)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x: input tensor shape=(batch, seq_len, hidden_dim=embedding_dim)\n",
        "        \"\"\"\n",
        "        x = self.layernorm1(x + self.mma(x))\n",
        "        x = self.layernorm2(x + self.mlp(x))\n",
        "        return x\n",
        "\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bXNhLPmo9sTG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 20, 100])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "# more efficient, buffer for pe version\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, device:str=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "        L = self.max_len\n",
        "        partial_term = torch.outer(torch.arange(L),1/10_000**(torch.arange(torch.ceil(torch.tensor(self.d_model/2)))*2/self.d_model))\n",
        "        positional_encoding = torch.zeros((L, self.d_model)).to(device)\n",
        "        positional_encoding[:,::2] = torch.sin(partial_term.to(device))\n",
        "        positional_encoding[:,1::2] = torch.cos(partial_term.to(device))\n",
        "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        '''\n",
        "        x: Tensor, shape [batch, seq_len, embedding_dim]\n",
        "        '''\n",
        "        L = x.shape[1]\n",
        "        # print(self.device)\n",
        "\n",
        "        return self.dropout(x.to(self.device)+ self.positional_encoding[:L,:].to(self.device))\n",
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TransformerConfig, vocab_size:int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self.config.hidden_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=self.config.hidden_size,\n",
        "            dropout=self.config.dropout,\n",
        "            max_len=self.config.max_seq_len,\n",
        "            device=self.config.device\n",
        "        )\n",
        "        list_decoder_blocks = [DecoderBlock(config = self.config) \n",
        "                                    for _ in range(self.config.num_layers)]\n",
        "        self.decoder_blocks = nn.Sequential(*list_decoder_blocks)\n",
        "        self.final_layer_norm = nn.LayerNorm(normalized_shape=self.config.hidden_size,eps=self.config.layer_norm_epsilon)\n",
        "        # self.unembed = nn.Linear(self.config.hidden_size, config.vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.embedding(x)\n",
        "        # print(f\"x.shape={x.shape}\")\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.decoder_blocks(x)\n",
        "        x = self.final_layer_norm(x)\n",
        "        # x = self.unembed(x) # ,dim=2)\n",
        "        x = x @ self.embedding.weight.T\n",
        "        return x\n",
        "    \n",
        "config = TransformerConfig(\n",
        "    num_layers=2,\n",
        "    num_heads=4,\n",
        "    vocab_size=1_000,\n",
        "    hidden_size=64,\n",
        "    max_seq_len=100,\n",
        "    device=device\n",
        ")\n",
        "# todo use the layernorm epsilon\n",
        "test_input = torch.ones((2,20)).int().to(config.device)\n",
        "\n",
        "transformer = DecoderOnlyTransformer(config=config, vocab_size=100)\n",
        "transformer.to(config.device)\n",
        "result = transformer.forward(test_input)\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Kw-NPEQh9sTG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss, MSELoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjLl3kSH9sTH"
      },
      "outputs": [],
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "    # def __init__(self, text, labels):\n",
        "    #     self.labels = labels\n",
        "    #     self.text = text\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        self.config = config\n",
        "        self.seq_len = 25\n",
        "        self.total_size = 1000\n",
        "        self.vocab_size=config.vocab_size\n",
        "        self.text = torch.rand((self.seq_len)).int().to(config.device).repeat(self.total_size,1)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "            return self.total_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "            label = self.text[idx,1:]\n",
        "            text = self.text[idx,:-1]\n",
        "            return text, label\n",
        "\n",
        "class ReverseDataset(Dataset):\n",
        "    # def __init__(self, text, labels):\n",
        "    #     self.labels = labels\n",
        "    #     self.text = text\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        self.config = config\n",
        "        self.seq_len = 25\n",
        "        self.total_size = 1000\n",
        "        self.vocab_size=config.vocab_size\n",
        "        self.text = torch.rand((self.seq_len)).int().to(config.device).repeat(self.total_size,1)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "            return self.total_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "            label = self.text[idx]\n",
        "            text = self.text[idx]\n",
        "            return text, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn1RAB0-9sTH",
        "outputId": "6ce1bc6e-a732-4e4e-9951-3a51026310f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config: TransformerConfig(num_layers=2, num_heads=4, vocab_size=50257, hidden_size=64, max_seq_len=100, dropout=0.1, layer_norm_epsilon=1e-05, device='cpu')\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "step must be greater than zero",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [49], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig: \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m model \u001b[39m=\u001b[39m train(config)\n",
            "Cell \u001b[0;32mIn [49], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      8\u001b[0m criterion \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mfor\u001b[39;00m i, (text, target_label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     11\u001b[0m         \u001b[39m# print(i, batch[\"text\"].shape, batch[\"label\"].shape)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         label \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(text)\n\u001b[1;32m     13\u001b[0m         \u001b[39m# target_label=batch[\"label\"]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[39m# print(f\"{label.shape=} {target_label.shape=}\")\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn [43], line 39\u001b[0m, in \u001b[0;36mReverseDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     38\u001b[0m         label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext[idx,:]\n\u001b[0;32m---> 39\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext[idx,::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[1;32m     40\u001b[0m         sample \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: text, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m: label}\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m text, label\n",
            "\u001b[0;31mValueError\u001b[0m: step must be greater than zero"
          ]
        }
      ],
      "source": [
        "def train(config: TransformerConfig):\n",
        "    dataset = ReverseDataset(config)\n",
        "    model = DecoderOnlyTransformer(config, vocab_size=config.vocab_size).to(config.device)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    model.train()\n",
        "    optimizer = Adam(params=model.parameters(), lr=0.001)\n",
        "    # criterion = CrossEntropyLoss()\n",
        "    criterion = CrossEntropyLoss()\n",
        "    for epoch_idx in range(5):\n",
        "        for i, (text, target_label) in enumerate(dataloader):\n",
        "            # print(i, batch[\"text\"].shape, batch[\"label\"].shape)\n",
        "            label = model.forward(text)\n",
        "            # target_label=batch[\"label\"]\n",
        "            # print(f\"{label.shape=} {target_label.shape=}\")\n",
        "            loss = criterion(label.transpose(1,2), target_label.long())\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            # if i %  == 0:\n",
        "        # print(f\"torch.mean(label)={torch.mean(label)} \\t torch.mean(target_label)={torch.mean(target_label)}\")\n",
        "        print(loss.detach().cpu().numpy())\n",
        "\n",
        "    return model\n",
        "\n",
        "print(f\"config: {config}\")\n",
        "\n",
        "model = train(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 10, 1000])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsHjn8q-9sTH"
      },
      "outputs": [],
      "source": [
        "import shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To9tIKo-Lexc"
      },
      "outputs": [],
      "source": [
        "config = TransformerConfig(\n",
        "    num_layers=2,\n",
        "    num_heads=4,\n",
        "    vocab_size=shakespeare.vocab_size,\n",
        "    hidden_size=64,\n",
        "    max_seq_len=100,\n",
        "    device=device\n",
        ")\n",
        "# dataset = shakespeare.ShakespeareDataset(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = shakespeare.ShakespeareDataset(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def collate(batch: list):\n",
        "#     print(batch)\n",
        "#     max_len = max([len(text) for (text, label) in batch])\n",
        "#     batch_size = len(batch)\n",
        "#     new_batch = list()\n",
        "#     for i, (text, label) in enumerate(batch):\n",
        "#         padded_text = torch.zeros(max_len).long()\n",
        "#         padded_text[:len(text)]=text\n",
        "#         padded_label = torch.zeros(max_len).long()\n",
        "#         padded_label[:len(label)] = label\n",
        "#         new_batch.append((padded_text, padded_label))\n",
        "#     return new_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate(batch: list):\n",
        "    # print(batch)\n",
        "    device = batch[0][0].device\n",
        "    max_len = max([len(text) for (text, label) in batch])\n",
        "    batch_size = len(batch)\n",
        "    new_text = torch.zeros((batch_size, max_len)).long().to(device)\n",
        "    new_label = torch.zeros((batch_size, max_len)).long().to(device)\n",
        "    for i, (text, label) in enumerate(batch):\n",
        "        new_text[i,:len(text)]=text\n",
        "        new_label[i,:len(label)] = label\n",
        "    return new_text, new_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:on2rk6k1) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▂▁▂▂▂▁▂▂▂▁▂▂▁▂▁▁▁▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>1.60911</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">masked-witch-5</strong>: <a href=\"https://wandb.ai/dalasnoin/arena-w1/runs/on2rk6k1\" target=\"_blank\">https://wandb.ai/dalasnoin/arena-w1/runs/on2rk6k1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221031_180056-on2rk6k1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:on2rk6k1). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "172b0f72ceb94be486917d623bc4db7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016751663883330062, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/YaoLu/src/github.com/dalasnoin/arena/w1/wandb/run-20221031_211038-1g070ea9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/dalasnoin/arena-w1/runs/1g070ea9\" target=\"_blank\">mischievous-witch-6</a></strong> to <a href=\"https://wandb.ai/dalasnoin/arena-w1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [40], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> 33\u001b[0m model \u001b[39m=\u001b[39m train(config, dataset)\n",
            "Cell \u001b[0;32mIn [40], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config, dataset)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# target_label=batch[\"label\"]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# print(f\"{label.shape=} {target_label.shape=}\")\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m criterion(label\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m), target_label)\n\u001b[0;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/wandb/wandb_torch.py:282\u001b[0m, in \u001b[0;36mTorchHistory._hook_variable_gradient_stats.<locals>.<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_tensor_stats(grad\u001b[39m.\u001b[39mdata, name)\n\u001b[0;32m--> 282\u001b[0m handle \u001b[39m=\u001b[39m var\u001b[39m.\u001b[39mregister_hook(\u001b[39mlambda\u001b[39;00m grad: _callback(grad, log_track))\n\u001b[1;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_handles[name] \u001b[39m=\u001b[39m handle\n\u001b[1;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m handle\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "wandb.init()\n",
        "def train(config: TransformerConfig, dataset: Dataset):\n",
        "    \n",
        "    model = DecoderOnlyTransformer(config, vocab_size=dataset.vocab_size).to(config.device)\n",
        "    wandb.watch(model,log_freq=100)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
        "    model.train()\n",
        "    optimizer = Adam(params=model.parameters(), lr=0.001)\n",
        "    # criterion = CrossEntropyLoss()\n",
        "    criterion = CrossEntropyLoss()\n",
        "    for epoch_idx in range(1):\n",
        "        for i, (text, target_label) in enumerate(dataloader):\n",
        "            # print(i, batch[\"text\"].shape, batch[\"label\"].shape)\n",
        "            label = model.forward(text)\n",
        "            # target_label=batch[\"label\"]\n",
        "            # print(f\"{label.shape=} {target_label.shape=}\")\n",
        "            loss = criterion(label.transpose(1,2), target_label)\n",
        "           \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            if i % 100 == 0:\n",
        "               wandb.log({\"loss\": loss})\n",
        "        # print(f\"torch.mean(label)={torch.mean(label)} \\t torch.mean(target_label)={torch.mean(target_label)}\")\n",
        "        loss_numpy = loss.detach().cpu().numpy()\n",
        "        print(loss_numpy)\n",
        "        if loss_numpy < 1.0:\n",
        "            break\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "model = train(config, dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = shakespeare.ShakespeareDataset(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([19])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from https://medium.com/@ml_kid/how-to-save-our-model-to-google-drive-and-reuse-it-2c1028058cb2\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# !mkdir -p /content/gdrive/My\\ Drive/colab/models\n",
        "model_save_name = 'classifier.pt'\n",
        "path = F\"/content/gdrive/My Drive/colab/models/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)\n",
        "\n",
        "# # load\n",
        "# model_save_name = 'classifier.pt'\n",
        "# path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "# model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text, label = dataset[0]\n",
        "text = text[None,...]\n",
        "label = label[None, ...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[36101, 35915, 36101,  9408, 10682, 50202, 37442, 45907,  3231, 50202,\n",
              "         45611, 42418, 39703, 45114, 15748, 15748, 50202, 45114]])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.forward(text).argmin(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ulum',\n",
              " 'ulum',\n",
              " 'ĠVed',\n",
              " '477',\n",
              " 'yssey',\n",
              " 'Ġderegulation',\n",
              " 'ĠGG',\n",
              " 'udos',\n",
              " 'Ġderegulation',\n",
              " 'obi',\n",
              " 'Ġ1902',\n",
              " 'udos',\n",
              " 'ĠAoE',\n",
              " 'Ġ1902',\n",
              " 'ometry',\n",
              " 'Ġferment',\n",
              " 'Ġ1902',\n",
              " 'Ġ1902']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shakespeare.tokenizer.convert_ids_to_tokens(model.forward(text)[0].argmin(-1).detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('science')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ded5e6f133e31c74d7e61946920be103f96969c2c9abd403ec1a6f8823efeff2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
