{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.ones((2,100,64))\n",
    "K = torch.ones((2,90,64))\n",
    "V = torch.ones((2,90,64))\n",
    "\n",
    "\n",
    "def attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (batch, target sequence length, embedding dim)\n",
    "    K: shape (batch, source sequence length, embedding dim)\n",
    "    V: shape (batch, source sequence length, embedding dim)\n",
    "    softmax(Q KT/sqrt(d_k))V\n",
    "\n",
    "    Return: shape (same as Q if embedding dim same. batch, target sequence length, output embedding dim)\n",
    "    '''\n",
    "    sqrt_d_k = torch.sqrt(torch.tensor(K.shape[-1]))\n",
    "    query_key = torch.bmm(Q,torch.transpose(K,1,2))\n",
    "    # print(f\"{query_key.shape=} {sqrt_d_k=}\")\n",
    "    result =torch.bmm(softmax(query_key/sqrt_d_k,dim=2), V)\n",
    "    return result\n",
    "\n",
    "attention(Q, K, V).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n",
      "tensor([[[64., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., 64., -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., 64., 64., -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., -inf],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.]],\n",
      "\n",
      "        [[64., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., -inf, -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., 64., -inf, -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., 64., 64., -inf, -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., -inf, -inf],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., -inf],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
      "         [64., 64., 64., 64., 64., 64., 64., 64., 64., 64.]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Q = torch.ones((2,20,64))\n",
    "K = torch.ones((2,10,64))\n",
    "V = torch.ones((2,10,64))\n",
    "\n",
    "def masked_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "    '''\n",
    "    Should return the results of self-attention.\n",
    "\n",
    "    You should implement masking for this function. See \"The Decoder Side\" for an explanation of masking.\n",
    "\n",
    "    Q: shape (batch, target sequence length, embedding dim)\n",
    "    K: shape (batch, source sequence length, embedding dim)\n",
    "    V: shape (batch, source sequence length, embedding dim)\n",
    "    I = Q K.T\n",
    "    I.shape = target_len x source_len\n",
    "    softmax((I+mask)/sqrt(d_k))V\n",
    "\n",
    "    Return: shape (same as Q if embedding dim same. batch, target sequence length, output embedding dim)\n",
    "    '''\n",
    "    sqrt_d_k = torch.sqrt(torch.tensor(K.shape[-1]))\n",
    "    target_seq_len = torch.tensor(Q.shape[1])\n",
    "    source_seq_len = torch.tensor(K.shape[1])\n",
    "    triangular = torch.triu(torch.ones((target_seq_len, source_seq_len), dtype=torch.bool), diagonal=1)\n",
    "    print(triangular)\n",
    "\n",
    "    query_key = torch.bmm(Q, torch.transpose(K,1,2))\n",
    "    masked_query_key = torch.where(triangular, -torch.inf, query_key)\n",
    "    print(masked_query_key)\n",
    "    result =torch.bmm(softmax((masked_query_key)/sqrt_d_k,dim=2), V)\n",
    "    return triangular\n",
    "\n",
    "\n",
    "\n",
    "triangular = masked_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "plt.imshow(triangular.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.dtype' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\simon\\src\\github.com\\dalasnoin\\arena\\w1d1\\attention.ipynb Zelle 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/simon/src/github.com/dalasnoin/arena/w1d1/attention.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mbool(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'torch.dtype' object is not callable"
     ]
    }
   ],
   "source": [
    "torch.bool(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a75635f6916c375a173bf1244d5cfd48b57dc00ad122fc43f351e9ec98f7b18f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
