{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qeufDKPr9sS_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "from modules import PositionalEncoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuer Abschnitt"
      ],
      "metadata": {
        "id": "c9iXrqhs9wDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mlWXh8id9sTB"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlW4UMtp9sTC",
        "outputId": "66729385-b1e3-4fd1-ad23-4173b398aabe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "Q = torch.ones((2,100,64))\n",
        "K = torch.ones((2,90,64))\n",
        "V = torch.ones((2,90,64))\n",
        "\n",
        "\n",
        "def attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
        "    '''\n",
        "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
        "\n",
        "    With this function, you can ignore masking.\n",
        "\n",
        "    Q: shape (batch, target sequence length, embedding dim)\n",
        "    K: shape (batch, source sequence length, embedding dim)\n",
        "    V: shape (batch, source sequence length, embedding dim)\n",
        "    softmax(Q KT/sqrt(d_k))V\n",
        "\n",
        "    Return: shape (same as Q if embedding dim same. batch, target sequence length, output embedding dim)\n",
        "    '''\n",
        "    sqrt_d_k = torch.sqrt(torch.tensor(K.shape[-1]))\n",
        "    query_key = torch.bmm(Q,torch.transpose(K,1,2))\n",
        "    # print(f\"{query_key.shape=} {sqrt_d_k=}\")\n",
        "    result =torch.bmm(softmax(query_key/sqrt_d_k,dim=2), V)\n",
        "    return result\n",
        "\n",
        "attention(Q, K, V).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6GJAW-I9sTD",
        "outputId": "2e98bfac-90f1-42f9-9bcb-b695c7d3d1eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = torch.ones((2,20,64))\n",
        "K = torch.ones((2,10,64))\n",
        "V = torch.ones((2,10,64))\n",
        "\n",
        "def masked_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
        "    '''\n",
        "    Should return the results of self-attention.\n",
        "\n",
        "    You should implement masking for this function. See \"The Decoder Side\" for an explanation of masking.\n",
        "\n",
        "    Q: shape (batch, target sequence length, embedding dim)\n",
        "    K: shape (batch, source sequence length, embedding dim)\n",
        "    V: shape (batch, source sequence length, embedding dim)\n",
        "    I = Q K.T\n",
        "    I.shape = target_len x source_len\n",
        "    softmax((I+mask)/sqrt(d_k))V\n",
        "\n",
        "    Return: shape (same as Q if embedding dim same. batch, target sequence length, output embedding dim)\n",
        "    '''\n",
        "    sqrt_d_k = torch.sqrt(torch.tensor(K.shape[-1]))\n",
        "    target_seq_len = torch.tensor(Q.shape[1])\n",
        "    source_seq_len = torch.tensor(K.shape[1])\n",
        "    triangular = torch.triu(torch.ones((target_seq_len, source_seq_len), dtype=torch.bool), diagonal=1)\n",
        "    # print(triangular)\n",
        "\n",
        "    query_key = torch.bmm(Q, torch.transpose(K,1,2))\n",
        "    masked_query_key = torch.where(triangular, -torch.inf, query_key)\n",
        "    # print(masked_query_key.shape, query_key.shape, triangular.shape)\n",
        "    result =torch.bmm(softmax((masked_query_key)/sqrt_d_k,dim=2), V)\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "result = masked_attention(Q, K, V)\n",
        "print(result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KABaU-mM9sTD"
      },
      "outputs": [],
      "source": [
        "# from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uJSNJynz9sTE"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(triangular.detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQVh_Q7k9sTE",
        "outputId": "12f81628-b437-4ea8-f2f2-85f2d59afc64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 20, 256])\n"
          ]
        }
      ],
      "source": [
        "Q = torch.ones((2,20,4*64))\n",
        "K = torch.ones((2,10,4*64))\n",
        "V = torch.ones((2,10,4*64))\n",
        "num_heads = 4\n",
        "\n",
        "def multihead_masked_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, num_heads: int, device:str=\"cpu\"):\n",
        "    '''\n",
        "    Implements multihead masked attention on the matrices Q, K and V.\n",
        "\n",
        "    Q: shape (batch, seq, nheads*headsize)\n",
        "    K: shape (batch, seq, nheads*headsize)\n",
        "    V: shape (batch, seq, nheads*headsize)\n",
        "    '''\n",
        "    # do the reshape\n",
        "    \n",
        "    batch, target_seq_len = Q.shape[0:2]\n",
        "    source_seq_len = K.shape[1] \n",
        "    head_size = int(Q.shape[-1]/num_heads)\n",
        "    sqrt_d_k = torch.sqrt(torch.tensor(head_size))\n",
        "    # new_shape = (batch, target_seq_len, num_heads, head_size)\n",
        "    Q = torch.reshape(Q, (batch, target_seq_len, num_heads, head_size))\n",
        "    K = torch.reshape(K, (batch, source_seq_len, num_heads, head_size))\n",
        "    V = torch.reshape(V, (batch, source_seq_len, num_heads, head_size))\n",
        "    # generate mask\n",
        "    triangular = torch.triu(torch.ones((target_seq_len, source_seq_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    \n",
        "    query_key = torch.einsum(\"abcd,aecd->acbe\", Q, K)\n",
        "    masked_query_key = torch.where(triangular, -torch.inf, query_key)\n",
        "    masked_query_key = softmax((masked_query_key)/sqrt_d_k,dim=1)\n",
        "    result = torch.einsum(\"abcd, adbe-> acbe\", query_key, V)\n",
        "    result = torch.reshape(result, (batch, target_seq_len, num_heads * head_size))\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "result = multihead_masked_attention(Q, K, V, num_heads=num_heads)\n",
        "print(result.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2TyEhq8w9sTF"
      },
      "outputs": [],
      "source": [
        "class MultiheadMaskedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    head_size is not in this config, because in our implementation we're assuming num_heads * head_size = hidden_size.\n",
        "hidden_size is also referred to as embedding_dim, or d_\\text{model}d \n",
        "model\n",
        "â€‹\n",
        "  in some material you might have read.\n",
        "    \"\"\"\n",
        "    W_QKV: nn.Linear\n",
        "    W_O: nn.Linear\n",
        "\n",
        "\n",
        "    def __init__(self, hidden_size: int, num_heads: int, device:str=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        self.W_QKV = nn.Linear(hidden_size*3, num_heads*hidden_size*3)\n",
        "        self.W_O = nn.Linear(num_heads*hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        '''\n",
        "        x: shape (batch, seq, hidden_size)\n",
        "\n",
        "        Return: shape (batch, seq, hidden_size)\n",
        "        '''\n",
        "        x = x.repeat((1,1,3)) # repeat trice along dim 2\n",
        "        Q, K, V = torch.split(self.W_QKV(x), num_heads*self.hidden_size, 2)\n",
        "        #print(f\"{Q.shape=} {K.shape=} {V.shape=}\")\n",
        "        \n",
        "        Z = multihead_masked_attention(Q, K, V, num_heads=self.num_heads, device=self.device)\n",
        "        #print(f\"{Z.shape=}\")\n",
        "        Z = self.W_O(Z)\n",
        "        return Z\n",
        "\n",
        "# num_heads=4\n",
        "# x = torch.ones((2,10,hidden_size:=64))       \n",
        "# mma = MultiheadMaskedAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
        "# mma(x).shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7agfiLCs9sTF"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TransformerConfig:\n",
        "    '''Constants used throughout your decoder-only transformer model.'''\n",
        "\n",
        "    num_layers: int\n",
        "    # head_size is not in this config, because in our implementation we're assuming num_heads * head_size = hidden_size\n",
        "    num_heads: int\n",
        "    vocab_size: int\n",
        "    # hidden_size is also referred to as embedding_dim, or d_\\text{model}d model in some material you might have read.\n",
        "    hidden_size: int\n",
        "    # max_seq_len is used just to determine the size of the positional encoding matrix.\n",
        "    max_seq_len: int \n",
        "    dropout: float = 0.1\n",
        "    layer_norm_epsilon: float = 1e-05\n",
        "    device: str = \"cpu\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CqiplUqF9sTF"
      },
      "outputs": [],
      "source": [
        "from torch.nn import GELU\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_size: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = dropout\n",
        "        self.mlp_block = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size, 4*self.hidden_size),\n",
        "            GELU(),\n",
        "            nn.Linear(4*self.hidden_size, self.hidden_size),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.mlp_block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wBPTdYRG9sTG"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "        self.layernorm1 = nn.LayerNorm(normalized_shape=self.config.hidden_size,eps=self.config.layer_norm_epsilon)\n",
        "        self.mma = MultiheadMaskedAttention(hidden_size=self.config.hidden_size,\n",
        "                                            num_heads=self.config.num_heads,\n",
        "                                            device=self.device)\n",
        "        self.layernorm2 = nn.LayerNorm(normalized_shape=self.config.hidden_size,eps=self.config.layer_norm_epsilon)\n",
        "        self.mlp = MLP(hidden_size=self.config.hidden_size,\n",
        "                        dropout=self.config.dropout)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x: input tensor shape=(batch, seq_len, hidden_dim=embedding_dim)\n",
        "        \"\"\"\n",
        "        x = self.layernorm1(x + self.mma(x))\n",
        "        x = self.layernorm2(x + self.mlp(x))\n",
        "        return x\n",
        "\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bXNhLPmo9sTG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "# more efficient, buffer for pe version\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, device:str=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "        L = self.max_len\n",
        "        partial_term = torch.outer(torch.arange(L),1/10_000**(torch.arange(torch.ceil(torch.tensor(self.d_model/2)))*2/self.d_model))\n",
        "        positional_encoding = torch.zeros((L, self.d_model)).to(device)\n",
        "        positional_encoding[:,::2] = torch.sin(partial_term.to(device))\n",
        "        positional_encoding[:,1::2] = torch.cos(partial_term.to(device))\n",
        "        self.register_buffer(\"positional_encoding\", positional_encoding)\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        '''\n",
        "        x: Tensor, shape [batch, seq_len, embedding_dim]\n",
        "        '''\n",
        "        L = x.shape[1]\n",
        "        # print(self.device)\n",
        "\n",
        "        return self.dropout(x.to(self.device)+ self.positional_encoding[:L,:].to(self.device))\n",
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=self.config.hidden_size,\n",
        "            dropout=self.config.dropout,\n",
        "            max_len=self.config.max_seq_len,\n",
        "            device=self.config.device\n",
        "        )\n",
        "        list_decoder_blocks = [DecoderBlock(config = self.config) \n",
        "                                    for _ in range(self.config.num_layers)]\n",
        "        self.decoder_blocks = nn.Sequential(*list_decoder_blocks)\n",
        "        self.final_layer_norm = nn.LayerNorm(normalized_shape=self.config.hidden_size,eps=self.config.layer_norm_epsilon)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.decoder_blocks(x)\n",
        "        x = self.final_layer_norm(x)\n",
        "        return x\n",
        "    \n",
        "config = TransformerConfig(\n",
        "    num_layers=2,\n",
        "    num_heads=4,\n",
        "    vocab_size=1_000,\n",
        "    hidden_size=64,\n",
        "    max_seq_len=100,\n",
        "    device=\"cuda:0\"\n",
        ")\n",
        "# todo use the layernorm epsilon\n",
        "test_input = torch.ones((2,20,config.hidden_size)).to(config.device)\n",
        "\n",
        "transformer = DecoderOnlyTransformer(config=config)\n",
        "transformer.to(config.device)\n",
        "result = transformer.forward(test_input)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Kw-NPEQh9sTG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss, MSELoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "EjLl3kSH9sTH"
      },
      "outputs": [],
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "    # def __init__(self, text, labels):\n",
        "    #     self.labels = labels\n",
        "    #     self.text = text\n",
        "    def __init__(self, config: TransformerConfig):\n",
        "        self.config = config\n",
        "        self.seq_len = 25\n",
        "        self.total_size = 800\n",
        "        # self.text = torch.ones((self.total_size,\n",
        "        #                         self.seq_len,\n",
        "        #                         config.hidden_size)).to(config.device)\n",
        "        # self.labels = torch.ones((self.total_size,\n",
        "        #                         self.seq_len,\n",
        "        #                         config.hidden_size)).to(config.device)\n",
        "        self.text = torch.rand((self.seq_len,\n",
        "                                config.hidden_size)).to(config.device).repeat(self.total_size,1,1)\n",
        "        self.labels = torch.rand((self.seq_len,\n",
        "                                config.hidden_size)).to(config.device).repeat(self.total_size,1,1)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "            return self.total_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "            label = self.labels[idx]\n",
        "            text = self.text[idx]\n",
        "            sample = {\"text\": text, \"label\": label}\n",
        "            return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn1RAB0-9sTH",
        "outputId": "6ce1bc6e-a732-4e4e-9951-3a51026310f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.mean(label)=0.010626577772200108 \t torch.mean(target_label)=0.499437540769577\n",
            "1.1457098\n",
            "torch.mean(label)=0.02143881842494011 \t torch.mean(target_label)=0.499437540769577\n",
            "1.0673913\n",
            "torch.mean(label)=0.03147929534316063 \t torch.mean(target_label)=0.499437540769577\n",
            "0.98414993\n",
            "torch.mean(label)=0.04172204062342644 \t torch.mean(target_label)=0.499437540769577\n",
            "0.90472853\n",
            "torch.mean(label)=0.052106063812971115 \t torch.mean(target_label)=0.499437540769577\n",
            "0.82592636\n",
            "torch.mean(label)=0.06351444870233536 \t torch.mean(target_label)=0.499437540769577\n",
            "0.74642915\n",
            "torch.mean(label)=0.07574815303087234 \t torch.mean(target_label)=0.499437540769577\n",
            "0.6665242\n",
            "torch.mean(label)=0.09068012982606888 \t torch.mean(target_label)=0.499437540769577\n",
            "0.59633225\n",
            "torch.mean(label)=0.10547496378421783 \t torch.mean(target_label)=0.499437540769577\n",
            "0.5259472\n",
            "torch.mean(label)=0.1238713413476944 \t torch.mean(target_label)=0.499437540769577\n",
            "0.46540505\n",
            "torch.mean(label)=0.14709745347499847 \t torch.mean(target_label)=0.499437540769577\n",
            "0.411704\n",
            "torch.mean(label)=0.17659629881381989 \t torch.mean(target_label)=0.499437540769577\n",
            "0.36138546\n",
            "torch.mean(label)=0.20815271139144897 \t torch.mean(target_label)=0.499437540769577\n",
            "0.3115048\n",
            "torch.mean(label)=0.2330276072025299 \t torch.mean(target_label)=0.499437540769577\n",
            "0.2622385\n",
            "torch.mean(label)=0.25048208236694336 \t torch.mean(target_label)=0.499437540769577\n",
            "0.22175455\n",
            "torch.mean(label)=0.26694944500923157 \t torch.mean(target_label)=0.499437540769577\n",
            "0.18930538\n",
            "torch.mean(label)=0.2818619906902313 \t torch.mean(target_label)=0.499437540769577\n",
            "0.16418384\n",
            "torch.mean(label)=0.29602116346359253 \t torch.mean(target_label)=0.499437540769577\n",
            "0.1448623\n",
            "torch.mean(label)=0.3084118962287903 \t torch.mean(target_label)=0.499437540769577\n",
            "0.12901397\n",
            "torch.mean(label)=0.3201407790184021 \t torch.mean(target_label)=0.499437540769577\n",
            "0.11763597\n",
            "torch.mean(label)=0.3333859145641327 \t torch.mean(target_label)=0.499437540769577\n",
            "0.108401075\n",
            "torch.mean(label)=0.34399211406707764 \t torch.mean(target_label)=0.499437540769577\n",
            "0.10073013\n",
            "torch.mean(label)=0.353860467672348 \t torch.mean(target_label)=0.499437540769577\n",
            "0.09450041\n",
            "torch.mean(label)=0.36358389258384705 \t torch.mean(target_label)=0.499437540769577\n",
            "0.08959989\n",
            "torch.mean(label)=0.37302908301353455 \t torch.mean(target_label)=0.499437540769577\n",
            "0.08539765\n",
            "torch.mean(label)=0.3816682696342468 \t torch.mean(target_label)=0.499437540769577\n",
            "0.08143196\n",
            "torch.mean(label)=0.3902124762535095 \t torch.mean(target_label)=0.499437540769577\n",
            "0.08686168\n",
            "torch.mean(label)=0.39773261547088623 \t torch.mean(target_label)=0.499437540769577\n",
            "0.077909976\n",
            "torch.mean(label)=0.4045763611793518 \t torch.mean(target_label)=0.499437540769577\n",
            "0.07334743\n",
            "torch.mean(label)=0.4111632704734802 \t torch.mean(target_label)=0.499437540769577\n",
            "0.06997216\n",
            "torch.mean(label)=0.41796568036079407 \t torch.mean(target_label)=0.499437540769577\n",
            "0.06809531\n",
            "torch.mean(label)=0.4241294860839844 \t torch.mean(target_label)=0.499437540769577\n",
            "0.066313386\n",
            "torch.mean(label)=0.4294370114803314 \t torch.mean(target_label)=0.499437540769577\n",
            "0.06259573\n",
            "torch.mean(label)=0.43438711762428284 \t torch.mean(target_label)=0.499437540769577\n",
            "0.059762485\n",
            "torch.mean(label)=0.43965286016464233 \t torch.mean(target_label)=0.499437540769577\n",
            "0.058312673\n",
            "torch.mean(label)=0.44399401545524597 \t torch.mean(target_label)=0.499437540769577\n",
            "0.055559695\n",
            "torch.mean(label)=0.4485645890235901 \t torch.mean(target_label)=0.499437540769577\n",
            "0.053660706\n",
            "torch.mean(label)=0.4525931477546692 \t torch.mean(target_label)=0.499437540769577\n",
            "0.051301096\n",
            "torch.mean(label)=0.45684462785720825 \t torch.mean(target_label)=0.499437540769577\n",
            "0.04910458\n",
            "torch.mean(label)=0.46006569266319275 \t torch.mean(target_label)=0.499437540769577\n",
            "0.047334757\n",
            "torch.mean(label)=0.4636383056640625 \t torch.mean(target_label)=0.499437540769577\n",
            "0.045372684\n",
            "torch.mean(label)=0.46692347526550293 \t torch.mean(target_label)=0.499437540769577\n",
            "0.042996887\n",
            "torch.mean(label)=0.47006210684776306 \t torch.mean(target_label)=0.499437540769577\n",
            "0.041473698\n",
            "torch.mean(label)=0.4732450246810913 \t torch.mean(target_label)=0.499437540769577\n",
            "0.040029354\n",
            "torch.mean(label)=0.4758155047893524 \t torch.mean(target_label)=0.499437540769577\n",
            "0.03911981\n",
            "torch.mean(label)=0.4787864089012146 \t torch.mean(target_label)=0.499437540769577\n",
            "0.037575264\n",
            "torch.mean(label)=0.481277734041214 \t torch.mean(target_label)=0.499437540769577\n",
            "0.03814441\n",
            "torch.mean(label)=0.4832665026187897 \t torch.mean(target_label)=0.499437540769577\n",
            "0.033833075\n",
            "torch.mean(label)=0.48573774099349976 \t torch.mean(target_label)=0.499437540769577\n",
            "0.03495317\n",
            "torch.mean(label)=0.4877919554710388 \t torch.mean(target_label)=0.499437540769577\n",
            "0.03399996\n"
          ]
        }
      ],
      "source": [
        "def train(config: TransformerConfig):\n",
        "    dataset = CustomTextDataset(config)\n",
        "    model = DecoderOnlyTransformer(config).to(config.device)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    model.train()\n",
        "    optimizer = Adam(params=model.parameters(), lr=0.001)\n",
        "    # criterion = CrossEntropyLoss()\n",
        "    criterion = MSELoss()\n",
        "    for epoch_idx in range(50):\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            # print(i, batch[\"text\"].shape, batch[\"label\"].shape)\n",
        "            label = model.forward(batch[\"text\"])\n",
        "            target_label=batch[\"label\"]\n",
        "            loss = criterion(label, target_label)\n",
        "           \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            # if i %  == 0:\n",
        "        print(f\"torch.mean(label)={torch.mean(label)} \\t torch.mean(target_label)={torch.mean(target_label)}\")\n",
        "        print(loss.detach().cpu().numpy())\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "model = train(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "jsHjn8q-9sTH"
      },
      "outputs": [],
      "source": [
        "criterion = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion(torch.ones(5),torch.ones(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVMaC_HfAhi4",
        "outputId": "0f25c023-2e0b-4be5-abbb-27652dcc0af3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.0472)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "To9tIKo-Lexc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('science')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a75635f6916c375a173bf1244d5cfd48b57dc00ad122fc43f351e9ec98f7b18f"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}