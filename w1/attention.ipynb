{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.ones((2,100,64))\n",
    "K = torch.ones((2,90,64))\n",
    "V = torch.ones((2,90,64))\n",
    "\n",
    "\n",
    "def attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (batch, target sequence length, embedding dim)\n",
    "    K: shape (batch, source sequence length, embedding dim)\n",
    "    V: shape (batch, source sequence length, embedding dim)\n",
    "    softmax(Q KT/sqrt(d_k))V\n",
    "\n",
    "    Return: shape (same as Q if embedding dim same. batch, target sequence length, output embedding dim)\n",
    "    '''\n",
    "    sqrt_d_k = torch.sqrt(torch.tensor(K.shape[-1]))\n",
    "    query_key = torch.bmm(Q,torch.transpose(K,1,2))\n",
    "    # print(f\"{query_key.shape=} {sqrt_d_k=}\")\n",
    "    result =torch.bmm(softmax(query_key/sqrt_d_k,dim=2), V)\n",
    "    return result\n",
    "\n",
    "attention(Q, K, V).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 10]) torch.Size([2, 20, 10]) torch.Size([20, 10])\n",
      "torch.Size([2, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Q = torch.ones((2,20,64))\n",
    "K = torch.ones((2,10,64))\n",
    "V = torch.ones((2,10,64))\n",
    "\n",
    "def masked_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "    '''\n",
    "    Should return the results of self-attention.\n",
    "\n",
    "    You should implement masking for this function. See \"The Decoder Side\" for an explanation of masking.\n",
    "\n",
    "    Q: shape (batch, target sequence length, embedding dim)\n",
    "    K: shape (batch, source sequence length, embedding dim)\n",
    "    V: shape (batch, source sequence length, embedding dim)\n",
    "    I = Q K.T\n",
    "    I.shape = target_len x source_len\n",
    "    softmax((I+mask)/sqrt(d_k))V\n",
    "\n",
    "    Return: shape (same as Q if embedding dim same. batch, target sequence length, output embedding dim)\n",
    "    '''\n",
    "    sqrt_d_k = torch.sqrt(torch.tensor(K.shape[-1]))\n",
    "    target_seq_len = torch.tensor(Q.shape[1])\n",
    "    source_seq_len = torch.tensor(K.shape[1])\n",
    "    triangular = torch.triu(torch.ones((target_seq_len, source_seq_len), dtype=torch.bool), diagonal=1)\n",
    "    # print(triangular)\n",
    "\n",
    "    query_key = torch.bmm(Q, torch.transpose(K,1,2))\n",
    "    masked_query_key = torch.where(triangular, -torch.inf, query_key)\n",
    "    # print(masked_query_key.shape, query_key.shape, triangular.shape)\n",
    "    result =torch.bmm(softmax((masked_query_key)/sqrt_d_k,dim=2), V)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "result = masked_attention(Q, K, V)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(triangular.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 256])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.ones((2,20,4*64))\n",
    "K = torch.ones((2,10,4*64))\n",
    "V = torch.ones((2,10,4*64))\n",
    "num_heads = 4\n",
    "\n",
    "def multihead_masked_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, num_heads: int):\n",
    "    '''\n",
    "    Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "    Q: shape (batch, seq, nheads*headsize)\n",
    "    K: shape (batch, seq, nheads*headsize)\n",
    "    V: shape (batch, seq, nheads*headsize)\n",
    "    '''\n",
    "    # do the reshape\n",
    "    \n",
    "    batch, target_seq_len = Q.shape[0:2]\n",
    "    source_seq_len = K.shape[1] \n",
    "    head_size = int(Q.shape[-1]/num_heads)\n",
    "    sqrt_d_k = torch.sqrt(torch.tensor(head_size))\n",
    "    # new_shape = (batch, target_seq_len, num_heads, head_size)\n",
    "    Q = torch.reshape(Q, (batch, target_seq_len, num_heads, head_size))\n",
    "    K = torch.reshape(K, (batch, source_seq_len, num_heads, head_size))\n",
    "    V = torch.reshape(V, (batch, source_seq_len, num_heads, head_size))\n",
    "    # generate mask\n",
    "    triangular = torch.triu(torch.ones((target_seq_len, source_seq_len), dtype=torch.bool), diagonal=1)\n",
    "    \n",
    "    query_key = torch.einsum(\"abcd,aecd->acbe\", Q, K)\n",
    "    masked_query_key = torch.where(triangular, -torch.inf, query_key)\n",
    "    masked_query_key = softmax((masked_query_key)/sqrt_d_k,dim=1)\n",
    "    result = torch.einsum(\"abcd, adbe-> acbe\", query_key, V)\n",
    "    result = torch.reshape(result, (batch, target_seq_len, num_heads * head_size))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "result = multihead_masked_attention(Q, K, V, num_heads=num_heads)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape=torch.Size([2, 10, 256]) K.shape=torch.Size([2, 10, 256]) V.shape=torch.Size([2, 10, 256])\n",
      "Z.shape=torch.Size([2, 10, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 64])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiheadMaskedAttention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_QKV = nn.Linear(hidden_size*3, num_heads*hidden_size*3)\n",
    "        self.W_O = nn.Linear(num_heads*hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        x = x.repeat((1,1,3)) # repeat trice along dim 2\n",
    "        Q, K, V = torch.split(self.W_QKV(x), num_heads*self.hidden_size, 2)\n",
    "        print(f\"{Q.shape=} {K.shape=} {V.shape=}\")\n",
    "        Z = multihead_masked_attention(Q, K, V, num_heads=self.num_heads)\n",
    "        print(f\"{Z.shape=}\")\n",
    "        Z = self.W_O(Z)\n",
    "        return Z\n",
    "\n",
    "# num_heads=4\n",
    "# x = torch.ones((2,10,hidden_size:=64))       \n",
    "# mma = MultiheadMaskedAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
    "# mma(x).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((1,2,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.repeat((1,1,3))\n",
    "torch.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a75635f6916c375a173bf1244d5cfd48b57dc00ad122fc43f351e9ec98f7b18f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
