{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w3d1/utils.py\n",
    "# ! pip install -U scikit-learn scipy\n",
    "import torch as t\n",
    "import utils\n",
    "from typing import Callable, Iterable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrocks_banana(x: t.Tensor, y: t.Tensor, a=1, b=100) -> t.Tensor:\n",
    "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
    "\n",
    "x_range = [-2, 2]\n",
    "y_range = [-1, 3]\n",
    "fig = utils.plot_fn(rosenbrocks_banana, x_range, y_range, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min is at (1,1) when both squared terms are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_fn_with_sgd(fn: Callable, xy: t.Tensor, lr=0.001, momentum=0.98, n_iters: int = 100):\n",
    "    '''\n",
    "    Optimize the a given function starting from the specified point.\n",
    "\n",
    "    xy: shape (2,). The (x, y) starting point.\n",
    "    n_iters: number of steps.\n",
    "\n",
    "    Return: (n_iters, 2). The (x,y) BEFORE each step. So out[0] is the starting point.\n",
    "    '''\n",
    "    assert xy.requires_grad\n",
    "    optimizer = optim.SGD([xy], lr=lr, momentum=momentum)\n",
    "    progression_curve = t.zeros((n_iters, len(xy)))\n",
    "    for i in range(n_iters):\n",
    "        progression_curve[i] = xy.detach()\n",
    "        result = fn(*xy)\n",
    "        result.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return progression_curve\n",
    "\n",
    "xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
    "x_range = [-2, 2]\n",
    "y_range = [-1, 3]\n",
    "\n",
    "fig = utils.plot_optimization_sgd(opt_fn_with_sgd, rosenbrocks_banana, xy, x_range, y_range, lr=0.001, momentum=0.98, show_min=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    params: list\n",
    "\n",
    "    def __init__(self, params: Iterable[t.nn.parameter.Parameter], lr: float, momentum: float, weight_decay: float):\n",
    "        '''Implements SGD with momentum.\n",
    "\n",
    "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
    "        '''\n",
    "        self.params = list(params)\n",
    "\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {momentum}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {weight_decay}\")\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.timestep = 0\n",
    "\n",
    "        self.gradient_updates = [t.zeros_like(param) for param in self.params]\n",
    "\n",
    "    def step(self) -> None:\n",
    "        for t, (gradient_update, param) in enumerate(zip(self.gradient_updates, self.params)):\n",
    "            grads = param.grad\n",
    "            if self.weight_decay != 0:\n",
    "                grads = grads + self.weight_decay*param\n",
    "            if self.momentum != 0 and self.timestep > 1:\n",
    "                # I wonder if this is correct, i thought it should be (1-momentum)*grads\n",
    "                grads = self.momentum * gradient_update + grads\n",
    "            \n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # Should return something reasonable here, e.g. \"SGD(lr=lr, ...)\"\n",
    "        return f\"SGD lr={self.lr} momentum={self.momentum} weight_decay={self.weight_decay}\"\n",
    "\n",
    "utils.test_sgd(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a75635f6916c375a173bf1244d5cfd48b57dc00ad122fc43f351e9ec98f7b18f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
