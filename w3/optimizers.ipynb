{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w3d1/utils.py\n",
    "# ! pip install -U scikit-learn scipy\n",
    "import torch as t\n",
    "import utils\n",
    "from typing import Callable, Iterable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrocks_banana(x: t.Tensor, y: t.Tensor, a=1, b=100) -> t.Tensor:\n",
    "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
    "\n",
    "x_range = [-2, 2]\n",
    "y_range = [-1, 3]\n",
    "fig = utils.plot_fn(rosenbrocks_banana, x_range, y_range, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min is at (1,1) when both squared terms are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_fn_with_sgd(fn: Callable, xy: t.Tensor, lr=0.001, momentum=0.98, n_iters: int = 100):\n",
    "    '''\n",
    "    Optimize the a given function starting from the specified point.\n",
    "\n",
    "    xy: shape (2,). The (x, y) starting point.\n",
    "    n_iters: number of steps.\n",
    "\n",
    "    Return: (n_iters, 2). The (x,y) BEFORE each step. So out[0] is the starting point.\n",
    "    '''\n",
    "    assert xy.requires_grad\n",
    "    optimizer = optim.SGD([xy], lr=lr, momentum=momentum)\n",
    "    progression_curve = t.zeros((n_iters, len(xy)))\n",
    "    for i in range(n_iters):\n",
    "        progression_curve[i] = xy.detach()\n",
    "        result = fn(*xy)\n",
    "        result.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return progression_curve\n",
    "\n",
    "xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
    "x_range = [-2, 2]\n",
    "y_range = [-1, 3]\n",
    "\n",
    "fig = utils.plot_optimization_sgd(opt_fn_with_sgd, rosenbrocks_banana, xy, x_range, y_range, lr=0.001, momentum=0.98, show_min=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    params: list\n",
    "\n",
    "    def __init__(self, params: Iterable[t.nn.parameter.Parameter], lr: float, momentum: float, weight_decay: float=0):\n",
    "        '''Implements SGD with momentum.\n",
    "\n",
    "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
    "        '''\n",
    "        self.params = list(params)\n",
    "\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {momentum}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {weight_decay}\")\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.timestep = 0\n",
    "\n",
    "        # same as self.gs on streamlit\n",
    "        self.gradient_updates = [t.zeros_like(p) for p in self.params]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Set param grads to None\n",
    "        \"\"\"\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "    def step(self) -> None:\n",
    "        with t.inference_mode():\n",
    "            for i, (gradient_update, param) in enumerate(zip(self.gradient_updates, self.params)):\n",
    "                grads = param.grad\n",
    "                if self.weight_decay != 0:\n",
    "                    grads += self.weight_decay*param\n",
    "                if self.momentum != 0 and self.timestep > 0:\n",
    "                    # I wonder if this is correct, i thought it should be (1-momentum)*grads\n",
    "                    grads += self.momentum * gradient_update\n",
    "                self.params[i] -= self.lr * grads\n",
    "                self.gradient_updates[i]=grads\n",
    "            self.timestep += 1\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # Should return something reasonable here, e.g. \"SGD(lr=lr, ...)\"\n",
    "        return f\"SGD lr={self.lr} momentum={self.momentum} weight_decay={self.weight_decay}\"\n",
    "\n",
    "utils.test_sgd(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[t.nn.parameter.Parameter],\n",
    "        lr: float,\n",
    "        alpha: float,\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "        momentum: float,\n",
    "    ):\n",
    "        '''Implements RMSprop.\n",
    "\n",
    "        Like the PyTorch version, but assumes centered=False\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop\n",
    "        '''\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.moving_average_squared_gradients = [t.zeros_like(param) for param in self.params]\n",
    "        self.gradient_updates = [t.zeros_like(p) for p in self.params]\n",
    "\n",
    "        self.timestep = 0\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Set param grads to None\n",
    "        \"\"\"\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "    def step(self) -> None:\n",
    "        with t.inference_mode():\n",
    "            for i, (ma, gradient_update, param) in enumerate(zip(self.moving_average_squared_gradients, self.gradient_updates, self.params)):\n",
    "                grad = param.grad\n",
    "                if self.weight_decay != 0:\n",
    "                    grad += self.weight_decay*param\n",
    "\n",
    "                ma = self.alpha * ma + (1 - self.alpha) * grad ** 2\n",
    "                if self.momentum != 0:\n",
    "                    # I wonder if this is correct, i thought it should be (1-momentum)*grads\n",
    "                    gradient_update = self.momentum * gradient_update + grad / (t.sqrt(ma) + self.eps)\n",
    "                    self.params[i] -= self.lr * gradient_update\n",
    "                    self.gradient_updates[i] = gradient_update\n",
    "                else: \n",
    "                    \n",
    "                    self.params[i] -= self.lr / (t.sqrt(ma) + self.eps) * grad\n",
    "                self.moving_average_squared_gradients[i] = ma\n",
    "            self.timestep += 1\n",
    "\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"RMSprop lr = {self.lr}; alpha = {self.alpha}; weight_decay = {self.weight_decay}\"\n",
    "\n",
    "\n",
    "\n",
    "utils.test_rmsprop(RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[t.nn.parameter.Parameter],\n",
    "        lr: float,\n",
    "        betas: tuple[float, float],\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "    ):\n",
    "        '''Implements Adam.\n",
    "\n",
    "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n",
    "        '''\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        \n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.m = [t.zeros_like(param) for param in self.params]\n",
    "        self.v = [t.zeros_like(param) for param in self.params]\n",
    "\n",
    "        self.timestep = 1\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Set param grads to None\n",
    "        \"\"\"\n",
    "        for param in self.params:\n",
    "            param.grad = None\n",
    "\n",
    "    def step(self) -> None:\n",
    "        with t.inference_mode():\n",
    "            for i, (m, v, param) in enumerate(zip(self.m, self.v, self.params)):\n",
    "                grad = param.grad\n",
    "                if self.weight_decay != 0:\n",
    "                    grad += self.weight_decay * param\n",
    "                m = self.betas[0] * m + (1 - self.betas[0]) * grad\n",
    "                v = self.betas[1] * v + (1-self.betas[1]) * grad ** 2\n",
    "\n",
    "                \n",
    "                self.m[i] = m\n",
    "                self.v[i] = v\n",
    "\n",
    "                # adjust in the beginning of training\n",
    "                m_hat = m / (1 - self.betas[0]**self.timestep)\n",
    "                v_hat = v / (1 - self.betas[1]**self.timestep)\n",
    "\n",
    "\n",
    "                self.params[i] -= self.lr * m_hat / (t.sqrt(v_hat)+self.eps)\n",
    "\n",
    "            self.timestep += 1\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Adam lr = {self.lr} betas = {self.betas} weight_decay = {self.weight_decay}\"\n",
    "\n",
    "utils.test_adam(Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting multiple optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_fn(fn: Callable, xy: t.Tensor, optimizer_class, optimizer_kwargs, n_iters: int = 100):\n",
    "    '''Optimize the a given function starting from the specified point.\n",
    "\n",
    "    optimizer_class: one of the optimizers you've defined, either SGD, RMSprop, or Adam\n",
    "    optimzer_kwargs: keyword arguments passed to your optimiser (e.g. lr and weight_decay)\n",
    "    '''\n",
    "    assert xy.requires_grad\n",
    "    optimizer = optimizer_class([xy], **optimizer_kwargs)\n",
    "    progression_curve = t.zeros((n_iters, len(xy)))\n",
    "    for i in range(n_iters):\n",
    "        progression_curve[i] = xy.detach()\n",
    "        result = fn(*xy)\n",
    "        result.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return progression_curve\n",
    "\n",
    "xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
    "x_range = [-2, 2]\n",
    "y_range = [-1, 3]\n",
    "optimizer_kwargs = {\"lr\": 0.001, \"momentum\": 0.98}\n",
    "\n",
    "opt_fn(rosenbrocks_banana, xy, SGD, optimizer_kwargs=optimizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
    "x_range = [-2, 2]\n",
    "y_range = [-1, 3]\n",
    "optimizers = [\n",
    "    (SGD, dict(lr=1e-3, momentum=0.98)),\n",
    "    (SGD, dict(lr=5e-4, momentum=0.98)),\n",
    "]\n",
    "fn = rosenbrocks_banana\n",
    "fig = utils.plot_optimization(opt_fn, fn, xy, optimizers, x_range, y_range)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ded5e6f133e31c74d7e61946920be103f96969c2c9abd403ec1a6f8823efeff2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
