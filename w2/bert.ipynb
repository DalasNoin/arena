{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions: \n",
    "In appendix A.1 of the BERT paper, the masking procedure is described. 15% of the time the tokens are masked (meaning we process these tokens in some way, and perform gradient descent on the model's predictive loss on this token). However, masking doesn't always mean we replace the token with [MASK]. Of these 15% of cases, 80% of the time the word is replaced with [MASK], 10% of the time it is replaced with a random word, and 10% of the time it is kept unchanged. This is sometimes referred to as the 80-10-10 rule.\n",
    "Why is this used?\n",
    "\n",
    "On the one hand the model learns to understand text by filling out masks in the sentence. In a sentence like \" a MASK jumps over the fence \" it will learn which words can possibly be used here. on the other hand we don't want the model to just copy all of the other tokens but it essentially has to learn to spot strange words that dont't belong in the sentence. I assume gradient descent is only applied to the 15% \"masked\" tokens, so it also should sometimes get punished if it incorrectly changes a word, i.e. if it assumes a word was replaced by a random word. that is why gradient descent is also applied to unchanged tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers\n",
    "# ! pip install wandb\n",
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w2d2/utils.py\n",
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w2d2/solutions_build_bert.py\n",
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w2d2/functions_from_previous_days.py\n",
    "# ! wget https://raw.githubusercontent.com/DalasNoin/arena/main/w2/shakespeare.py\n",
    "# ! wget https://raw.githubusercontent.com/DalasNoin/arena/main/w2/sampling.py\n",
    "# ! pip install torchinfo\n",
    "\n",
    "import torch as t\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import GELU, Softmax\n",
    "from dataclasses import dataclass\n",
    "import transformers\n",
    "import utils\n",
    "from simon_utils import TransformerConfig\n",
    "import matplotlib\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "bert = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = TransformerConfig(\n",
    "    num_layers = 12,    # check bert.bert\n",
    "    num_heads = 12,     # check bert.bert\n",
    "    vocab_size = 28996, # tokenizer.vocab_size\n",
    "    hidden_size = 768,  # check bert.bert\n",
    "    max_seq_len = 512,  # bert.bert.embeddings.position_embeddings\n",
    "    dropout = 0.1,      # check bert.bert\n",
    "    layer_norm_epsilon = 1e-12, # check bert.bert\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](bert.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.dropout = config.dropout\n",
    "        self.mlp_block = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 4*self.hidden_size),\n",
    "            GELU(),\n",
    "            nn.Linear(4*self.hidden_size, self.hidden_size),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.mlp_block(x)\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super.__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, additive_attention_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        pass \n",
    "\n",
    "\n",
    "class BERTBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super.__init__()\n",
    "        self.attention = MultiheadAttention(config=config)\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=(config.hidden_size,))\n",
    "        self.mlp = MLP(config=config)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=(config.hidden_size,))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, additive_attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        additive_attention_mask: shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        '''\n",
    "        x += self.attention(x)\n",
    "        x = self.layernorm1(x)\n",
    "        x += self.mlp(x)\n",
    "        return self.layernorm2(x)\n",
    "\n",
    "\n",
    "\n",
    "def make_additive_attention_mask(one_zero_attention_mask: torch.Tensor, big_negative_number: float = -10000) -> torch.Tensor:\n",
    "    '''\n",
    "    one_zero_attention_mask: \n",
    "        shape (batch, seq)\n",
    "        Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "    big_negative_number:\n",
    "        Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "    Out: \n",
    "        shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        Contains 0 if attention is allowed, big_negative_number if not.\n",
    "    '''\n",
    "    return (big_negative_number * ~one_zero_attention_mask)[:, None, None, :]\n",
    "\n",
    "# utils.test_make_additive_attention_mask(make_additive_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCommon(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super.__init__()\n",
    "        self.token_embedding = nn.Embedding(num_embedding=config.vocab_size, embedding_dim=config.hidden_size)\n",
    "        self.position_embedding = nn.Embedding(num_embeddings=config.max_seq_len, embedding_dim=config.hidden_size)\n",
    "        self.token_type_embedding = nn.Embedding(num_embeddings=2, embedding_dim=config.hidden_size)\n",
    "        self.layernorm = nn.LayerNorm(normalized_shape=config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        blocks = [BERTBlock(config) for _ in range(config.num_layers)]\n",
    "        self.bert_blocks = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "\n",
    "        attention_mask = None\n",
    "        if one_zero_attention_mask:\n",
    "            attention_mask = make_additive_attention_mask(one_zero_attention_mask=one_zero_attention_mask)\n",
    "        \n",
    "        positions = t.arange(x.shape[1], device=self.config.device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = t.zeros(x.shape[1], device=self.config.device)\n",
    "        \n",
    "        x = self.token_embedding(x) + self.position_embedding(positions) + self.token_type_embedding(token_type_ids)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.bert_blocks(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BertLanguageModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_common = BertCommon(config)\n",
    "        self.linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.tied_unembed_bias = nn.Parameter(t.zeros(config.vocab_size))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights_from_bert(my_bert: BertLanguageModel, bert: transformers.models.bert.modeling_bert.BertForMaskedLM) -> BertLanguageModel:\n",
    "    '''\n",
    "    Copy over the weights from bert to your implementation of bert.\n",
    "\n",
    "    bert should be imported using: \n",
    "        bert = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    Returns your bert model, with weights loaded in.\n",
    "    '''\n",
    "\n",
    "    # FILL IN CODE: define a state dict from my_bert.named_parameters() and bert.named_parameters()\n",
    "\n",
    "    my_bert.load_state_dict(state_dict)\n",
    "    return my_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, text: str, k=15) -> List[List[str]]:\n",
    "    '''\n",
    "    Return a list of k strings for each [MASK] in the input.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def test_bert_prediction(predict, model, tokenizer):\n",
    "    '''Your Bert should know some names of American presidents.'''\n",
    "    text = \"Former President of the United States of America, George[MASK][MASK]\"\n",
    "    predictions = predict(model, tokenizer, text)\n",
    "    print(f\"Prompt: {text}\")\n",
    "    print(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))\n",
    "    assert \"Washington\" in predictions[0]\n",
    "    assert \"Bush\" in predictions[0]\n",
    "\n",
    "test_bert_prediction(predict, my_bert, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ded5e6f133e31c74d7e61946920be103f96969c2c9abd403ec1a6f8823efeff2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
