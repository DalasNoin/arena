{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DalasNoin/arena/blob/main/w2/gpt2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers\n",
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w2d2/utils.py\n",
    "# ! wget https://www.gutenberg.org/files/100/100-0.txt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import GELU, Softmax\n",
    "from dataclasses import dataclass\n",
    "import transformers\n",
    "import utils\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import GPT2Attention, GPT2BlockSimon, GPT2MLP, GPT2Model, TransformerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define gpt2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig()\n",
    "model = GPT2Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting some tools from week 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from itertools import zip_longest\n",
    "def compare_models(my_model, realmodel):\n",
    "    \n",
    "    pretraineddict = dict(realmodel.named_parameters())\n",
    "    my_state = dict(my_model.named_parameters())\n",
    "\n",
    "\n",
    "    keys_to_iterate = []\n",
    "\n",
    "    for pretrainedkey, pretrainedvalue in pretraineddict.items():\n",
    "        remove_these= [] #[\"attn.masked_bias\", \".attn.bias\", \"lm_head.weight\"]\n",
    "        for suffix in remove_these:\n",
    "            if pretrainedkey.endswith(suffix):\n",
    "                break\n",
    "        else:\n",
    "            keys_to_iterate.append(pretrainedkey)\n",
    "\n",
    "    # match keys\n",
    "    remaining_keys = list(my_state.keys())\n",
    "    matched_keys = []\n",
    "    for key in keys_to_iterate:\n",
    "        for mykey in remaining_keys:\n",
    "            if key.endswith(\"weight\") and not (key.endswith(\"wte.weight\") or key.endswith(\"wpe.weight\")):\n",
    "                # rotate shape\n",
    "                shape = pretraineddict[key].T.shape\n",
    "            else:\n",
    "                shape = pretraineddict[key].shape\n",
    "\n",
    "            myshape = my_state[mykey].shape\n",
    "            if shape == myshape:\n",
    "                matched_keys.append(mykey)\n",
    "                remaining_keys.remove(mykey)\n",
    "                break\n",
    "        else: \n",
    "            print(f\"No match found for key {key}\")\n",
    "            \n",
    "    print(f\"remaining keys = {remaining_keys}\")\n",
    "\n",
    "    print(f\"len(pretraineddict)={len(keys_to_iterate)}\\tlen(my_state)={len(my_state)}\")\n",
    "    utils.print_param_count(my_model, realmodel)\n",
    "    \n",
    "    df = pd.DataFrame.from_records(\n",
    "        [(tk, tuple(pretraineddict[tk].shape), mk, tuple(my_state[mk].shape)) for (tk, mk) in zip(keys_to_iterate, matched_keys)],\n",
    "        columns=[\"their name\", \"their shape\", \"your name\", \"your shape\"],\n",
    "    )\n",
    "    # if len(pretraineddict)!= len(my_state):\n",
    "        # for tk, tv in pretraineddict.items():\n",
    "        #     print(f\"{tk}\\t{tuple(tv.shape)}\")\n",
    "    \n",
    "        # for tk, tv in my_state.items():\n",
    "        #     print(f\"{tk}\\t{tuple(tv.shape)}\")\n",
    "\n",
    "    with pd.option_context(\"display.max_rows\", None):  # type: ignore\n",
    "        display(df)\n",
    "\n",
    "#  compare_models(model,gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied params: transformer.wte.weight -> text_embedding.weight\n",
      "Copied params: transformer.wpe.weight -> position_embedding.weight\n",
      "Copied params.T: transformer.h.0.ln_1.weight -> decoder_blocks.0.ln1.weight\n",
      "Copied params: transformer.h.0.ln_1.bias -> decoder_blocks.0.ln1.bias\n",
      "Copied params.T: transformer.h.0.attn.c_attn.weight -> decoder_blocks.0.attn.W_QKV.weight\n",
      "Copied params: transformer.h.0.attn.c_attn.bias -> decoder_blocks.0.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.0.attn.c_proj.weight -> decoder_blocks.0.attn.W_O.weight\n",
      "Copied params: transformer.h.0.attn.c_proj.bias -> decoder_blocks.0.attn.W_O.bias\n",
      "Copied params.T: transformer.h.0.ln_2.weight -> decoder_blocks.0.ln2.weight\n",
      "Copied params: transformer.h.0.ln_2.bias -> decoder_blocks.0.ln2.bias\n",
      "Copied params.T: transformer.h.0.mlp.c_fc.weight -> decoder_blocks.0.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.0.mlp.c_fc.bias -> decoder_blocks.0.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.0.mlp.c_proj.weight -> decoder_blocks.0.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.0.mlp.c_proj.bias -> decoder_blocks.0.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.1.ln_1.weight -> decoder_blocks.1.ln1.weight\n",
      "Copied params: transformer.h.1.ln_1.bias -> decoder_blocks.1.ln1.bias\n",
      "Copied params.T: transformer.h.1.attn.c_attn.weight -> decoder_blocks.1.attn.W_QKV.weight\n",
      "Copied params: transformer.h.1.attn.c_attn.bias -> decoder_blocks.1.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.1.attn.c_proj.weight -> decoder_blocks.1.attn.W_O.weight\n",
      "Copied params: transformer.h.1.attn.c_proj.bias -> decoder_blocks.1.attn.W_O.bias\n",
      "Copied params.T: transformer.h.1.ln_2.weight -> decoder_blocks.1.ln2.weight\n",
      "Copied params: transformer.h.1.ln_2.bias -> decoder_blocks.1.ln2.bias\n",
      "Copied params.T: transformer.h.1.mlp.c_fc.weight -> decoder_blocks.1.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.1.mlp.c_fc.bias -> decoder_blocks.1.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.1.mlp.c_proj.weight -> decoder_blocks.1.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.1.mlp.c_proj.bias -> decoder_blocks.1.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.2.ln_1.weight -> decoder_blocks.2.ln1.weight\n",
      "Copied params: transformer.h.2.ln_1.bias -> decoder_blocks.2.ln1.bias\n",
      "Copied params.T: transformer.h.2.attn.c_attn.weight -> decoder_blocks.2.attn.W_QKV.weight\n",
      "Copied params: transformer.h.2.attn.c_attn.bias -> decoder_blocks.2.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.2.attn.c_proj.weight -> decoder_blocks.2.attn.W_O.weight\n",
      "Copied params: transformer.h.2.attn.c_proj.bias -> decoder_blocks.2.attn.W_O.bias\n",
      "Copied params.T: transformer.h.2.ln_2.weight -> decoder_blocks.2.ln2.weight\n",
      "Copied params: transformer.h.2.ln_2.bias -> decoder_blocks.2.ln2.bias\n",
      "Copied params.T: transformer.h.2.mlp.c_fc.weight -> decoder_blocks.2.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.2.mlp.c_fc.bias -> decoder_blocks.2.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.2.mlp.c_proj.weight -> decoder_blocks.2.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.2.mlp.c_proj.bias -> decoder_blocks.2.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.3.ln_1.weight -> decoder_blocks.3.ln1.weight\n",
      "Copied params: transformer.h.3.ln_1.bias -> decoder_blocks.3.ln1.bias\n",
      "Copied params.T: transformer.h.3.attn.c_attn.weight -> decoder_blocks.3.attn.W_QKV.weight\n",
      "Copied params: transformer.h.3.attn.c_attn.bias -> decoder_blocks.3.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.3.attn.c_proj.weight -> decoder_blocks.3.attn.W_O.weight\n",
      "Copied params: transformer.h.3.attn.c_proj.bias -> decoder_blocks.3.attn.W_O.bias\n",
      "Copied params.T: transformer.h.3.ln_2.weight -> decoder_blocks.3.ln2.weight\n",
      "Copied params: transformer.h.3.ln_2.bias -> decoder_blocks.3.ln2.bias\n",
      "Copied params.T: transformer.h.3.mlp.c_fc.weight -> decoder_blocks.3.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.3.mlp.c_fc.bias -> decoder_blocks.3.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.3.mlp.c_proj.weight -> decoder_blocks.3.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.3.mlp.c_proj.bias -> decoder_blocks.3.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.4.ln_1.weight -> decoder_blocks.4.ln1.weight\n",
      "Copied params: transformer.h.4.ln_1.bias -> decoder_blocks.4.ln1.bias\n",
      "Copied params.T: transformer.h.4.attn.c_attn.weight -> decoder_blocks.4.attn.W_QKV.weight\n",
      "Copied params: transformer.h.4.attn.c_attn.bias -> decoder_blocks.4.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.4.attn.c_proj.weight -> decoder_blocks.4.attn.W_O.weight\n",
      "Copied params: transformer.h.4.attn.c_proj.bias -> decoder_blocks.4.attn.W_O.bias\n",
      "Copied params.T: transformer.h.4.ln_2.weight -> decoder_blocks.4.ln2.weight\n",
      "Copied params: transformer.h.4.ln_2.bias -> decoder_blocks.4.ln2.bias\n",
      "Copied params.T: transformer.h.4.mlp.c_fc.weight -> decoder_blocks.4.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.4.mlp.c_fc.bias -> decoder_blocks.4.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.4.mlp.c_proj.weight -> decoder_blocks.4.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.4.mlp.c_proj.bias -> decoder_blocks.4.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.5.ln_1.weight -> decoder_blocks.5.ln1.weight\n",
      "Copied params: transformer.h.5.ln_1.bias -> decoder_blocks.5.ln1.bias\n",
      "Copied params.T: transformer.h.5.attn.c_attn.weight -> decoder_blocks.5.attn.W_QKV.weight\n",
      "Copied params: transformer.h.5.attn.c_attn.bias -> decoder_blocks.5.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.5.attn.c_proj.weight -> decoder_blocks.5.attn.W_O.weight\n",
      "Copied params: transformer.h.5.attn.c_proj.bias -> decoder_blocks.5.attn.W_O.bias\n",
      "Copied params.T: transformer.h.5.ln_2.weight -> decoder_blocks.5.ln2.weight\n",
      "Copied params: transformer.h.5.ln_2.bias -> decoder_blocks.5.ln2.bias\n",
      "Copied params.T: transformer.h.5.mlp.c_fc.weight -> decoder_blocks.5.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.5.mlp.c_fc.bias -> decoder_blocks.5.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.5.mlp.c_proj.weight -> decoder_blocks.5.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.5.mlp.c_proj.bias -> decoder_blocks.5.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.6.ln_1.weight -> decoder_blocks.6.ln1.weight\n",
      "Copied params: transformer.h.6.ln_1.bias -> decoder_blocks.6.ln1.bias\n",
      "Copied params.T: transformer.h.6.attn.c_attn.weight -> decoder_blocks.6.attn.W_QKV.weight\n",
      "Copied params: transformer.h.6.attn.c_attn.bias -> decoder_blocks.6.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.6.attn.c_proj.weight -> decoder_blocks.6.attn.W_O.weight\n",
      "Copied params: transformer.h.6.attn.c_proj.bias -> decoder_blocks.6.attn.W_O.bias\n",
      "Copied params.T: transformer.h.6.ln_2.weight -> decoder_blocks.6.ln2.weight\n",
      "Copied params: transformer.h.6.ln_2.bias -> decoder_blocks.6.ln2.bias\n",
      "Copied params.T: transformer.h.6.mlp.c_fc.weight -> decoder_blocks.6.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.6.mlp.c_fc.bias -> decoder_blocks.6.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.6.mlp.c_proj.weight -> decoder_blocks.6.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.6.mlp.c_proj.bias -> decoder_blocks.6.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.7.ln_1.weight -> decoder_blocks.7.ln1.weight\n",
      "Copied params: transformer.h.7.ln_1.bias -> decoder_blocks.7.ln1.bias\n",
      "Copied params.T: transformer.h.7.attn.c_attn.weight -> decoder_blocks.7.attn.W_QKV.weight\n",
      "Copied params: transformer.h.7.attn.c_attn.bias -> decoder_blocks.7.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.7.attn.c_proj.weight -> decoder_blocks.7.attn.W_O.weight\n",
      "Copied params: transformer.h.7.attn.c_proj.bias -> decoder_blocks.7.attn.W_O.bias\n",
      "Copied params.T: transformer.h.7.ln_2.weight -> decoder_blocks.7.ln2.weight\n",
      "Copied params: transformer.h.7.ln_2.bias -> decoder_blocks.7.ln2.bias\n",
      "Copied params.T: transformer.h.7.mlp.c_fc.weight -> decoder_blocks.7.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.7.mlp.c_fc.bias -> decoder_blocks.7.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.7.mlp.c_proj.weight -> decoder_blocks.7.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.7.mlp.c_proj.bias -> decoder_blocks.7.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.8.ln_1.weight -> decoder_blocks.8.ln1.weight\n",
      "Copied params: transformer.h.8.ln_1.bias -> decoder_blocks.8.ln1.bias\n",
      "Copied params.T: transformer.h.8.attn.c_attn.weight -> decoder_blocks.8.attn.W_QKV.weight\n",
      "Copied params: transformer.h.8.attn.c_attn.bias -> decoder_blocks.8.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.8.attn.c_proj.weight -> decoder_blocks.8.attn.W_O.weight\n",
      "Copied params: transformer.h.8.attn.c_proj.bias -> decoder_blocks.8.attn.W_O.bias\n",
      "Copied params.T: transformer.h.8.ln_2.weight -> decoder_blocks.8.ln2.weight\n",
      "Copied params: transformer.h.8.ln_2.bias -> decoder_blocks.8.ln2.bias\n",
      "Copied params.T: transformer.h.8.mlp.c_fc.weight -> decoder_blocks.8.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.8.mlp.c_fc.bias -> decoder_blocks.8.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.8.mlp.c_proj.weight -> decoder_blocks.8.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.8.mlp.c_proj.bias -> decoder_blocks.8.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.9.ln_1.weight -> decoder_blocks.9.ln1.weight\n",
      "Copied params: transformer.h.9.ln_1.bias -> decoder_blocks.9.ln1.bias\n",
      "Copied params.T: transformer.h.9.attn.c_attn.weight -> decoder_blocks.9.attn.W_QKV.weight\n",
      "Copied params: transformer.h.9.attn.c_attn.bias -> decoder_blocks.9.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.9.attn.c_proj.weight -> decoder_blocks.9.attn.W_O.weight\n",
      "Copied params: transformer.h.9.attn.c_proj.bias -> decoder_blocks.9.attn.W_O.bias\n",
      "Copied params.T: transformer.h.9.ln_2.weight -> decoder_blocks.9.ln2.weight\n",
      "Copied params: transformer.h.9.ln_2.bias -> decoder_blocks.9.ln2.bias\n",
      "Copied params.T: transformer.h.9.mlp.c_fc.weight -> decoder_blocks.9.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.9.mlp.c_fc.bias -> decoder_blocks.9.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.9.mlp.c_proj.weight -> decoder_blocks.9.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.9.mlp.c_proj.bias -> decoder_blocks.9.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.10.ln_1.weight -> decoder_blocks.10.ln1.weight\n",
      "Copied params: transformer.h.10.ln_1.bias -> decoder_blocks.10.ln1.bias\n",
      "Copied params.T: transformer.h.10.attn.c_attn.weight -> decoder_blocks.10.attn.W_QKV.weight\n",
      "Copied params: transformer.h.10.attn.c_attn.bias -> decoder_blocks.10.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.10.attn.c_proj.weight -> decoder_blocks.10.attn.W_O.weight\n",
      "Copied params: transformer.h.10.attn.c_proj.bias -> decoder_blocks.10.attn.W_O.bias\n",
      "Copied params.T: transformer.h.10.ln_2.weight -> decoder_blocks.10.ln2.weight\n",
      "Copied params: transformer.h.10.ln_2.bias -> decoder_blocks.10.ln2.bias\n",
      "Copied params.T: transformer.h.10.mlp.c_fc.weight -> decoder_blocks.10.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.10.mlp.c_fc.bias -> decoder_blocks.10.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.10.mlp.c_proj.weight -> decoder_blocks.10.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.10.mlp.c_proj.bias -> decoder_blocks.10.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.h.11.ln_1.weight -> decoder_blocks.11.ln1.weight\n",
      "Copied params: transformer.h.11.ln_1.bias -> decoder_blocks.11.ln1.bias\n",
      "Copied params.T: transformer.h.11.attn.c_attn.weight -> decoder_blocks.11.attn.W_QKV.weight\n",
      "Copied params: transformer.h.11.attn.c_attn.bias -> decoder_blocks.11.attn.W_QKV.bias\n",
      "Copied params.T: transformer.h.11.attn.c_proj.weight -> decoder_blocks.11.attn.W_O.weight\n",
      "Copied params: transformer.h.11.attn.c_proj.bias -> decoder_blocks.11.attn.W_O.bias\n",
      "Copied params.T: transformer.h.11.ln_2.weight -> decoder_blocks.11.ln2.weight\n",
      "Copied params: transformer.h.11.ln_2.bias -> decoder_blocks.11.ln2.bias\n",
      "Copied params.T: transformer.h.11.mlp.c_fc.weight -> decoder_blocks.11.mlp.mlp_block.0.weight\n",
      "Copied params: transformer.h.11.mlp.c_fc.bias -> decoder_blocks.11.mlp.mlp_block.0.bias\n",
      "Copied params.T: transformer.h.11.mlp.c_proj.weight -> decoder_blocks.11.mlp.mlp_block.2.weight\n",
      "Copied params: transformer.h.11.mlp.c_proj.bias -> decoder_blocks.11.mlp.mlp_block.2.bias\n",
      "Copied params.T: transformer.ln_f.weight -> final_layer_norm.weight\n",
      "Copied params: transformer.ln_f.bias -> final_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "def copy_weights_simon(my_model: GPT2Model, pretrained_model: nn.Module) -> GPT2Model:\n",
    "    '''Copy over the weights from gpt to your implementation of gpt.\n",
    "\n",
    "    gpt should be imported using: \n",
    "        gpt = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "    Returns your gpt model, with weights loaded in.'''\n",
    "\n",
    "    pretraineddict = dict(pretrained_model.named_parameters())\n",
    "    my_state = dict(my_model.named_parameters())\n",
    "\n",
    "    keys_to_iterate = []\n",
    "\n",
    "    for pretrainedkey, pretrainedvalue in pretraineddict.items():\n",
    "        remove_these= [\"attn.masked_bias\", \".attn.bias\", \"lm_head.weight\"]\n",
    "        for suffix in remove_these:\n",
    "            if pretrainedkey.endswith(suffix):\n",
    "                break\n",
    "        else:\n",
    "            keys_to_iterate.append(pretrainedkey)\n",
    "\n",
    "    # match keys\n",
    "    remaining_keys = list(my_state.keys())\n",
    "    matched_keys = {}\n",
    "    for key in keys_to_iterate:\n",
    "        for mykey in remaining_keys:\n",
    "            if key.endswith(\"weight\") and not (key.endswith(\"wte.weight\") or key.endswith(\"wpe.weight\")):\n",
    "                # transpose shape\n",
    "                pretrained_values = pretraineddict[key].T\n",
    "                print(f\"Copied params.T: {key} -> {mykey}\")\n",
    "            else:\n",
    "                pretrained_values = pretraineddict[key]\n",
    "                print(f\"Copied params: {key} -> {mykey}\")\n",
    "\n",
    "            myshape = my_state[mykey].shape\n",
    "            if pretrained_values.shape == myshape:\n",
    "                matched_keys[mykey] = pretrained_values\n",
    "                remaining_keys.remove(mykey)\n",
    "                break\n",
    "        else: \n",
    "            print(f\"No match found for key {key}\")\n",
    "\n",
    "\n",
    "    # Check the number of params/buffers is correct\n",
    "    assert len(matched_keys) == len(keys_to_iterate), \"Number of layers is wrong. Have you done the prev step correctly?\"\n",
    "\n",
    "    # Initialise an empty dictionary to store the correct key-value pairs\n",
    "    # state_dict_to_load = {}\n",
    "\n",
    "    # for mykey, pretrainedkey in zip(matched_keys, keys_to_iterate):\n",
    "    #     pretrainedvalue = pretraineddict[pretrainedkey]\n",
    "    #     state_dict_to_load[mykey] = pretrainedvalue\n",
    "\n",
    "    my_model.load_state_dict(matched_keys)\n",
    "\n",
    "    return my_model\n",
    "\n",
    "my_gpt = copy_weights_simon(model, gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Former President of the United States of America, George\n",
      "Your model's top 10 predictions:  [' W', ' H', ' Bush', ' Washington', ' HW', ' Herbert', ' Pat', ' Soros', ' S', ' Wallace']\n",
      "Prompt:  Former President of the United States of America, George\n",
      "Your model's top 10 predictions:  [' W', ' H', ' Bush', ' Washington', ' HW', ' Herbert', ' Pat', ' S', ' Soros', ' Wallace']\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "def test_load_pretrained_weights(model, tokenizer):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    def encode(text: str) -> t.Tensor:\n",
    "        \"\"\"Return a Tensor of shape (batch=1, seq).\"\"\"\n",
    "        return tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "    prompt = \"Former President of the United States of America, George\"\n",
    "    input_ids = encode(prompt)\n",
    "    with t.inference_mode():\n",
    "        output = model(input_ids)\n",
    "        logits = output[0, -1] if isinstance(output, t.Tensor) else output.logits[0, -1]\n",
    "    topk = t.topk(logits, k=10).indices\n",
    "    next_tokens = tokenizer.batch_decode(topk.reshape(-1, 1))\n",
    "    print(\"Prompt: \", prompt)\n",
    "    print(\"Your model's top 10 predictions: \", next_tokens)\n",
    "    assert \" Washington\" in next_tokens\n",
    "    assert \" Bush\" in next_tokens\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "test_load_pretrained_weights(my_gpt, tokenizer)\n",
    "test_load_pretrained_weights(gpt2, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sampling\n",
    "import shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a group of unicorns passing their tabbot into a plantation held in the rear yard of a second estate of Firestone S. Butler and his brothers. Butler realized, however, that taking a long standing shamrooming tradition to its logical end could only get them anything more than a scrap of land they needed to replace bolt-in slaves and others she didn't want\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling.sample_tokens(\n",
    "    model=my_gpt,\n",
    "    tokenizer=tokenizer,\n",
    "    initial_text=\"a group of unicorns\",\n",
    "    max_tokens_generated=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = shakespeare.ShakespeareDataset(config, use_word_tokenizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ohg6q1w) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49bb2bca0f04f7596f55f81269d3d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.007 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.090230…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">silver-music-1</strong>: <a href=\"https://wandb.ai/dalasnoin/arena-w2/runs/3ohg6q1w\" target=\"_blank\">https://wandb.ai/dalasnoin/arena-w2/runs/3ohg6q1w</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221104_144256-3ohg6q1w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ohg6q1w). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e361bdcb2bc044888046aada1dbc7011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016727997916670272, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/YaoLu/src/github.com/dalasnoin/arena/w2/wandb/run-20221104_144322-11y2awsz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dalasnoin/arena-w2/runs/11y2awsz\" target=\"_blank\">sage-cherry-2</a></strong> to <a href=\"https://wandb.ai/dalasnoin/arena-w2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 63\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> 63\u001b[0m model \u001b[39m=\u001b[39m train(config, dataset, my_gpt)\n",
      "Cell \u001b[0;32mIn [42], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config, dataset, model)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m# target_label=batch[\"label\"]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# print(f\"{label.shape=} {target_label.shape=}\")\u001b[39;00m\n\u001b[1;32m     47\u001b[0m loss \u001b[39m=\u001b[39m criterion(label\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m), target_label)\n\u001b[0;32m---> 49\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     50\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     51\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/science/lib/python3.9/site-packages/wandb/wandb_torch.py:282\u001b[0m, in \u001b[0;36mTorchHistory._hook_variable_gradient_stats.<locals>.<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_tensor_stats(grad\u001b[39m.\u001b[39mdata, name)\n\u001b[0;32m--> 282\u001b[0m handle \u001b[39m=\u001b[39m var\u001b[39m.\u001b[39mregister_hook(\u001b[39mlambda\u001b[39;00m grad: _callback(grad, log_track))\n\u001b[1;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_handles[name] \u001b[39m=\u001b[39m handle\n\u001b[1;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m handle\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"arena week 2\"\n",
    "wandb_key = \"\"\n",
    "keyfile = \"keystore.yaml\"\n",
    "if not wandb_key and os.path.exists(keyfile):\n",
    "    import yaml\n",
    "    keys = yaml.safe_load(open(keyfile,\"r\"))\n",
    "    wandb_key = keys[\"wandb\"]\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "\n",
    "def collate(batch: list):\n",
    "    # print(batch)\n",
    "    device = batch[0][0].device\n",
    "    max_len = max([len(text) for (text, label) in batch])\n",
    "    batch_size = len(batch)\n",
    "    new_text = torch.zeros((batch_size, max_len)).long().to(device)\n",
    "    new_label = torch.zeros((batch_size, max_len)).long().to(device)\n",
    "    for i, (text, label) in enumerate(batch):\n",
    "        new_text[i,:len(text)]=text\n",
    "        new_label[i,:len(label)] = label\n",
    "    return new_text, new_label\n",
    "\n",
    "\n",
    "wandb.init(config=config.__dict__)\n",
    "def train(config: TransformerConfig, dataset: Dataset, model: GPT2Model):\n",
    "    model.train()\n",
    "    model.to(config.device)\n",
    "    wandb.watch(model,log_freq=100)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate)\n",
    "    model.train()\n",
    "    optimizer = AdamW(params=model.parameters(), lr=2e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,num_warmup_steps=200, num_training_steps=-1\n",
    "    )\n",
    "    # criterion = CrossEntropyLoss()\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch_idx in range(1):\n",
    "        for i, (text, target_label) in enumerate(dataloader):\n",
    "            # print(i, batch[\"text\"].shape, batch[\"label\"].shape)\n",
    "            label = model.forward(text)\n",
    "            # target_label=batch[\"label\"]\n",
    "            # print(f\"{label.shape=} {target_label.shape=}\")\n",
    "            loss = criterion(label.transpose(1,2), target_label)\n",
    "           \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if i % 100 == 0:\n",
    "               wandb.log({\"loss\": loss})\n",
    "        # print(f\"torch.mean(label)={torch.mean(label)} \\t torch.mean(target_label)={torch.mean(target_label)}\")\n",
    "        loss_numpy = loss.detach().cpu().numpy()\n",
    "        print(loss_numpy)\n",
    "        if loss_numpy < 1.0:\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = train(config, dataset, my_gpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 163.08it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model\u001b[39m=\u001b[39;49mmy_gpt, dataset\u001b[39m=\u001b[39;49mdataset, tokenizer\u001b[39m=\u001b[39;49mtokenizer)\n",
      "Cell \u001b[0;32mIn [39], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model, tokenizer, batch_size, epochs, lr, max_seq_len, warmup_steps, gpt2_type, output_dir, output_prefix, test_mode, save_model_on_epoch)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m idx, entry \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_dataloader)):\n\u001b[0;32m---> 44\u001b[0m     (input_tensor, carry_on, remainder) \u001b[39m=\u001b[39m pack_tensor(entry, input_tensor, \u001b[39m768\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m carry_on \u001b[39mand\u001b[39;00m idx \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     47\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [39], line 10\u001b[0m, in \u001b[0;36mpack_tensor\u001b[0;34m(new_tensor, packed_tensor, max_seq_len)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mif\u001b[39;00m packed_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m new_tensor, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mif\u001b[39;00m new_tensor\u001b[39m.\u001b[39;49msize()[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m packed_tensor\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m max_seq_len:\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m packed_tensor, \u001b[39mFalse\u001b[39;00m, new_tensor\n\u001b[1;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "train(model=my_gpt, dataset=dataset, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ded5e6f133e31c74d7e61946920be103f96969c2c9abd403ec1a6f8823efeff2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
