{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DalasNoin/arena/blob/main/w4/gan.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w4d1/utils.py\n",
    "# ! wget https://raw.githubusercontent.com/dalasnoin/arena/main/w4/gan_modules.py\n",
    "# ! gdown --id 1lfEWQ05cZ5FgWkSIxwyi7TLryhvnCDWb\n",
    "# ! mkdir data\n",
    "# ! unzip -qq img_align_celeba.zip -d data\n",
    "# ! pip install einops fancy_einsum tqdm plotly\n",
    "# ! pip install wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch as t\n",
    "from typing import Union\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from fancy_einsum import einsum\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import wandb\n",
    "import utils\n",
    "import gan_modules\n",
    "from typing import Optional\n",
    "\n",
    "image_size = img_size = 64 # i misleadingly used both names in the code\n",
    "batch_size = 3\n",
    "latent_dim_size = 100\n",
    "img_channels = 3\n",
    "generator_num_features = 512\n",
    "n_layers = 4\n",
    "\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "wandb_key=\"\"\n",
    "keyfile = \"keystore.yaml\"\n",
    "if not wandb_key and os.path.exists(keyfile):\n",
    "    import yaml\n",
    "    keys = yaml.safe_load(open(keyfile,\"r\"))\n",
    "    wandb_key = keys[\"wandb\"]\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @t.no_grad() not necessary since nn.init already uses nograd mode\n",
    "def initialize_weights(model: nn.Module) -> None:\n",
    "    \"\"\" \n",
    "    They mention at the end of page 3 that all weights were initialized from a N(0, 0.02)N(0,0.02) distribution. \n",
    "    This applies to the convolutional and convolutional transpose layers' weights, \n",
    "    but the BatchNorm layers' weights should be initialised from N(1, 0.02)N(1,0.02) (since 1 is their default value). \n",
    "    The BatchNorm biases should all be set to zero (which they are by default).\n",
    "    \"\"\"\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if \"batchnorm\" in name:\n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(parameter.data, 0.0)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.uniform_(parameter.data, a=0.02, b=1.0)\n",
    "        else:\n",
    "            nn.init.uniform_(parameter.data, a=0.0, b=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim_size: int,           # size of the random vector we use for generating outputs\n",
    "        img_size = int,                 # size of the images we're generating\n",
    "        img_channels = int,             # indicates RGB images\n",
    "        generator_num_features = int,   # number of channels after first projection and reshaping\n",
    "        n_layers = int,                 # number of CONV_n layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_dim_size = latent_dim_size\n",
    "        self.latent_dim_projected = 8192\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "        self.generator_num_features = generator_num_features\n",
    "        self.initial_width = 4\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "\n",
    "        self._build()\n",
    "\n",
    "\n",
    "    def _build(self):\n",
    "        self.latent_sequential = nn.Sequential(nn.Linear(self.latent_dim_size, self.latent_dim_projected, bias=False),\n",
    "            Rearrange(\"a (b c d) -> a b c d\", c=self.initial_width, d=self.initial_width),\n",
    "            nn.BatchNorm2d((self.latent_dim_projected//self.initial_width**2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer_structure = [(self.generator_num_features//(2**i),\n",
    "                                    self.generator_num_features//2**(i+1), \n",
    "                                    self.initial_width*2**i) for i in range(0, self.n_layers-1)]\n",
    "        self.layer_structure.append((self.img_size,self.img_channels,self.img_size))\n",
    "\n",
    "        block_list = [ConvTransposeBlock(*structure) for structure in self.layer_structure[:-1]]\n",
    "        block_list.append(ConvTransposeBlock(*self.layer_structure[-1],t.tanh, False))\n",
    "\n",
    "        self.upsample_sequential = nn.Sequential(*block_list)\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        x = self.latent_sequential(x)\n",
    "        return self.upsample_sequential(x)\n",
    "\n",
    "class ConvTransposeBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, width: int, activation_function: callable=None, uses_batchnorm:bool=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.width = width\n",
    "        self.convtranspose = gan_modules.ConvTranspose2d(in_channels=in_channels,\n",
    "                                            out_channels=out_channels,\n",
    "                                            kernel_size=4,\n",
    "                                            stride=2,\n",
    "                                            padding=1)\n",
    "        self.uses_batchnorm = uses_batchnorm\n",
    "        if uses_batchnorm:\n",
    "            self.batchnorm = nn.BatchNorm2d((self.out_channels))\n",
    "        if activation_function is None:\n",
    "            self.activation_function = nn.ReLU()\n",
    "        else:\n",
    "            self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convtranspose(x)\n",
    "        if self.uses_batchnorm:\n",
    "            x = self.batchnorm(x)\n",
    "        return self.activation_function(x)\n",
    "\n",
    "generator = Generator(\n",
    "    latent_dim_size=latent_dim_size,\n",
    "    img_size=image_size,\n",
    "    img_channels=img_channels,\n",
    "    generator_num_features=generator_num_features,\n",
    "    n_layers=n_layers\n",
    ")\n",
    "generator.layer_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size = 64,\n",
    "        img_channels = 3,\n",
    "        generator_num_features = 1024,\n",
    "        n_layers = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "        self.generator_num_features = generator_num_features\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # set as constants for now\n",
    "        self.conv_params = {\n",
    "            \"kernel_size\": 4,\n",
    "            \"stride\": 2,\n",
    "            \"padding\": 1,\n",
    "            \"bias\": False\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "        self.layer_structure = [(img_size*2**i, img_size*(2**(i+1)), img_size//2**(i+2)) for i in range(0, n_layers-1)]\n",
    "\n",
    "        self.initial_conv = nn.Conv2d(in_channels=self.img_channels, \n",
    "                                        out_channels=self.img_size,  # this seems to be the case for this model\n",
    "                                        **self.conv_params)\n",
    "        \n",
    "        self.downsampling_block = nn.Sequential(*[ConvBlock(*in_out_width_tuple,conv_params=self.conv_params) for in_out_width_tuple in self.layer_structure])\n",
    "\n",
    "        self.rearrange_layer = Rearrange(\"a b h w -> a (b h w)\")\n",
    "\n",
    "        self.classifier = nn.Linear(in_features=(self.layer_structure[-1][1]*self.layer_structure[-1][2]**2), # output size of the last layer\n",
    "                                    out_features=1,\n",
    "                                    bias=False\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.downsampling_block(x)\n",
    "        x = self.rearrange_layer(x)\n",
    "        return self.sigmoid(self.classifier(x))\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int, width:int, conv_params:dict, uses_batchnorm:bool=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.width = width\n",
    "        self.uses_batchnorm = uses_batchnorm\n",
    "\n",
    "        self.conv_params = conv_params\n",
    "\n",
    "        self.negative_slope=0.02\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            **self.conv_params\n",
    "        )\n",
    "\n",
    "        if self.uses_batchnorm:\n",
    "            self.batchnorm = nn.BatchNorm2d((out_channels))\n",
    "\n",
    "        self.leaky_relu = gan_modules.LeakyReLU(negative_slope=self.negative_slope)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.uses_batchnorm:\n",
    "            x = self.batchnorm(x)\n",
    "        return self.leaky_relu(x)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    img_size=img_size,\n",
    "    img_channels=img_channels,\n",
    "    generator_num_features=generator_num_features,\n",
    "    n_layers=n_layers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size,image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = ImageFolder(\n",
    "    root=\"data\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "utils.show_images(trainset, rows=3, cols=5)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_discriminator(\n",
    "    netG: Generator, \n",
    "    netD: Discriminator, \n",
    "    optG,\n",
    "    optD,\n",
    "    trainloader,\n",
    "    epochs: int,\n",
    "    max_epoch_duration: Optional[Union[int, float]] = None,           # Each epoch terminates after this many seconds\n",
    "    print_netG_output_interval: Optional[Union[int, float]] = None,   # Generator output is printed at this frequency\n",
    "    use_wandb: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "Discriminator:\n",
    "1. Zero the gradients of DD. This is important because if the last thing we did was evaluate D(G(z)) \n",
    "    (in order to update the parameters of G), then D will have stored gradients from that backward pass.\n",
    "\n",
    "Generate random noise z, and compute D(G(z)). Take the average of log(1−D(G(z))), \n",
    "and we have the first part of our loss function.\n",
    "\n",
    "Take the real images x in the current batch, \n",
    "and use that to compute log(D(x)) (we use this rather than log(1−D(x)), \n",
    "for reasons we'll discuss below). This gives us the second part of our loss function.\n",
    "\n",
    "We now add the two terms together, and perform gradient ascent (since we're trying to maximise this expression).\n",
    "You can perform gradient ascent by either flipping the sign of the thing you're doing a backward pass on, \n",
    "or passing the keyword argument maximize=True when defining your optimiser (all optimisers have this option).\n",
    "\n",
    "Tip - when calculating D(G(z)), for the purpose of training the discriminator, \n",
    "it's best to first calculate G(z) then call detach on this tensor before passing it to DD. \n",
    "This is because you then don't need to worry about gradients accumulating for GG.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.init()\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    netG.train().to(device)\n",
    "    netD.train().to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_images, labels) in enumerate(trainloader):\n",
    "            \n",
    "            real_images=real_images.to(device)\n",
    "            labels=labels.to(device)\n",
    "            # Discriminator training step\n",
    "            # 1.\n",
    "            optD.zero_grad()\n",
    "            # generate image from random\n",
    "            z = t.empty((batch_size, latent_dim_size)).uniform_(0,1).to(device)\n",
    "            \n",
    "            generated_images = netG(z).detach()\n",
    "            # use discriminator to get both components of the loss\n",
    "            loss_tensor = t.log(1-netD(generated_images) + 1e-5)\n",
    "            loss_generated_images = t.mean(loss_tensor)\n",
    "            \n",
    "            loss_real_images = t.mean(t.log(netD(real_images)))\n",
    "\n",
    "            loss_discriminator = loss_generated_images + loss_real_images\n",
    "\n",
    "            loss_discriminator.backward()\n",
    "\n",
    "            optD.step()\n",
    "\n",
    "\n",
    "            # Generator training step\n",
    "            \"\"\"Generator:\n",
    "We take the following steps:\n",
    "\n",
    "Zero the gradients of GG.\n",
    "Generate random noise zz, and compute D(G(z)).\n",
    "We don't use log(1−D(G(z))) to calculate our loss function, instead we use log(D(G(z))) (and gradient ascent).\n",
    "\"\"\"\n",
    "            optG.zero_grad()\n",
    "            z = t.empty((batch_size, latent_dim_size)).uniform_(0,1).to(device)\n",
    "            predictions = discriminator(generator(z))\n",
    "            loss_generator = t.log(t.mean(predictions))\n",
    "            loss_generator.backward()\n",
    "            \n",
    "            optG.step()\n",
    "\n",
    "            step += batch_size\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log(dict(epoch=epoch, loss_discriminator=loss_discriminator, loss_generator=loss_generator), step=step)\n",
    "            if use_wandb and step % 5000==0:\n",
    "                image_array = gan_modules.sample_generator_output(generator, 100)\n",
    "\n",
    "                images = Image.fromarray(np.uint8(image_array.numpy()*255))\n",
    "            \n",
    "                wandb.log({\"examples generated\": wandb.Image(images)})\n",
    "            \n",
    "    if use_wandb:\n",
    "        # upload models\n",
    "        filename = f\"{wandb.run.dir}/generator_state_dict.pt\"\n",
    "        print(f\"Saving model to: {filename}\")\n",
    "        t.save(generator.state_dict(), filename)\n",
    "        wandb.save(filename)       \n",
    "        filename = f\"{wandb.run.dir}/discriminator_state_dict.pt\"\n",
    "        print(f\"Saving model to: {filename}\")\n",
    "        t.save(discriminator.state_dict(), filename)\n",
    "        wandb.save(filename)  \n",
    "        wandb.finish()\n",
    "\n",
    "lr = 5e-5\n",
    "\n",
    "initialize_weights(generator)\n",
    "initialize_weights(discriminator)\n",
    "\n",
    "optimizer_generator = t.optim.Adam(generator.parameters(),lr=lr, maximize=True)\n",
    "optimizer_discriminator = t.optim.Adam(discriminator.parameters(),lr=lr, maximize=True)\n",
    "\n",
    "train_generator_discriminator(\n",
    "    netG=generator,\n",
    "    netD=discriminator,\n",
    "    optG=optimizer_generator,\n",
    "    optD=optimizer_discriminator,\n",
    "    trainloader=trainloader,\n",
    "    epochs=1,\n",
    "    use_wandb=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If stuck\n",
    "compare solutions to own version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w4d1/solutions.py\n",
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w4d1/w0d2_solutions.py\n",
    "# ! wget https://raw.githubusercontent.com/callummcdougall/arena-v1/main/w4d1/w0d3_solutions.py\n",
    "\n",
    "\n",
    "from solutions import netD_celeb_mini\n",
    "utils.print_param_count(discriminator, netD_celeb_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions import netG_celeb_mini\n",
    "utils.print_param_count(generator, netG_celeb_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "512*4*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(self.generator_num_features//(2**i),\n",
    "                                    self.generator_num_features//2**(i+1), \n",
    "                                    self.initial_width*2**i) for i in range(1, self.n_layers)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ded5e6f133e31c74d7e61946920be103f96969c2c9abd403ec1a6f8823efeff2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
